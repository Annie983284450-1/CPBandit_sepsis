{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9587ecfc-7dd6-475e-a072-e5cfc6edd00b",
   "metadata": {},
   "source": [
    "Modified from test_paper+supp.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e3bff3-095c-4744-9f21-e412bf1dc286",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Layer' from 'keras.engine' (c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\keras\\engine\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2608\\528767482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils_ECAD_journal\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils_ECAD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\98328\\Desktop\\cpbanditsepsis-1\\utils_ECAD_journal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\keras\\activations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Layer' from 'keras.engine' (c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\keras\\engine\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# !pip install pickle5\n",
    "# !pip install --upgrade --no-deps statsmodels\n",
    "# !pip install scikit-garden\n",
    "# !pip install scikit-learn==0.22.2 \n",
    "# Needed for scikit-garden to work properly. Restart runtime is NOT needed\n",
    "import skgarden \n",
    "import sys\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import time as time\n",
    "import utils_latest\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "from skgarden import RandomForestQuantileRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import utils_ECAD_journal as utils_ECAD\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import PI_class_EnbPI_journal as EnbPI\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg',force=True)\n",
    "from matplotlib import pyplot as plt\n",
    "print(\"Switched to:\",matplotlib.get_backend())\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.knn import KNN   # kNN detector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "import utils_EnbPI_journal as util\n",
    "from matplotlib.lines import Line2D  # For legend handles\n",
    "import calendar\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import itertools\n",
    "import importlib\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a013d263",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PI_class_EnbPI_journal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2608\\1368667230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PI_class_EnbPI_journal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PI_class_EnbPI_journal'"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(sys.modules['PI_class_EnbPI_journal'])\n",
    "\n",
    "import keras\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e01954-a198-4e68-ac10-2197e8026f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d144fc78",
   "metadata": {},
   "source": [
    "AttributeError: module 'tensorflow.compat.v2' has no attribute '__internal__'\n",
    "https://stackoverflow.com/questions/67696519/module-tensorflow-compat-v2-internal-has-no-attribute-tf2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d57fdc-da55-4bb5-b97f-f255e6c021b9",
   "metadata": {},
   "source": [
    "All the code is modified from tests_paper+supp_journal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7fefc4-d441-416d-8533-a5dbefb9e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably I do not need to do this for sepsis data\n",
    "def big_transform(CA_cities, current_city, one_dim, train_size):\n",
    "    # Used for California data\n",
    "    # Next, merge these data (so concatenate X_t and Y_t for one_d or not)\n",
    "    # Return [X_train, X_test, Y_train, Y_test] from data_x and data_y\n",
    "    # Data_x is either multivariate (direct concatenation)\n",
    "    # or univariate (transform each series and THEN concatenate the transformed series)\n",
    "    big_X_train = []\n",
    "    big_X_predict = []\n",
    "    for city in CA_cities:\n",
    "        data = eval(f'data{city}')  # Pandas DataFrame\n",
    "        data_x = data.loc[:, data.columns != 'DHI']\n",
    "        data_y = data['DHI']\n",
    "        data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "        data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "        X_train = data_x_numpy[:train_size, :]\n",
    "        X_predict = data_x_numpy[train_size:, :]\n",
    "        Y_train_del = data_y_numpy[:train_size]\n",
    "        Y_predict_del = data_y_numpy[train_size:]\n",
    "        if city == current_city:\n",
    "            Y_train = Y_train_del\n",
    "            Y_predict = Y_predict_del\n",
    "        if one_dim:\n",
    "            X_train, X_predict, Y_train_del, Y_predict_del = util.one_dimen_transform(\n",
    "                Y_train_del, Y_predict_del, d=20)\n",
    "            big_X_train.append(X_train)\n",
    "            big_X_predict.append(X_predict)\n",
    "            if city == current_city:\n",
    "                Y_train = Y_train_del\n",
    "        else:\n",
    "            big_X_train.append(X_train)\n",
    "            big_X_predict.append(X_predict)\n",
    "    X_train = np.hstack(big_X_train)\n",
    "    X_predict = np.hstack(big_X_predict)\n",
    "    return([X_train, X_predict, Y_train, Y_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f987af42-81fa-423a-90bd-38f77cbfdba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Latex_table_by_regr(array_ls, regr_name, methods_name, Data_names):\n",
    "    # first two columns store string: dataname and method\n",
    "    # last three columns store coverage, width, and Winkler score\n",
    "    if '1d' in regr_name:\n",
    "        array_t = array_ls\n",
    "        if len(Data_names) > 0:\n",
    "            np.savetxt(f\"cov_wid_score_{regr_name}_solar.txt\", array_t, fmt=(\n",
    "                '%1.2f', '%1.2f', '%.2e'), delimiter=' & ', newline=' \\\\\\\\\\n', comments='')\n",
    "        else:\n",
    "            np.savetxt(f\"cov_wid_score_{regr_name}.txt\", array_t, fmt=(\n",
    "                '%1.2f', '%1.2f', '%.2e'), delimiter=' & ', newline=' \\\\\\\\\\n', comments='')\n",
    "    else:\n",
    "        array_t = np.zeros(len(array_ls), dtype=(\n",
    "            '<U50,<U30, float64, float64, float64'))\n",
    "        multiplier = len(methods_name)\n",
    "        for j in range(len(Data_names)):\n",
    "            for k in range(multiplier):\n",
    "                jk = j * multiplier + k\n",
    "                remainder = np.mod(jk, multiplier)\n",
    "                if remainder == 0:\n",
    "                    if '1d' in regr_name:\n",
    "                        array_t[jk] = ' ', ' ', array_ls[jk, 0], array_ls[jk,\n",
    "                                                                          1], array_ls[jk, 2]\n",
    "                    else:\n",
    "                        name = '\\multirow' + \\\n",
    "                            '{' + f'{multiplier}' + '}' + '{*}' + \\\n",
    "                            '{' + f'{Data_names[j]}' + '}'\n",
    "                        array_t[jk] = name, methods_name[remainder], array_ls[jk,\n",
    "                                                                              0], array_ls[jk, 1], array_ls[jk, 2]\n",
    "                else:\n",
    "                    if '1d' in regr_name:\n",
    "                        array_t[jk] = ' ', ' ', array_ls[jk, 0], array_ls[jk,\n",
    "                                                                          1], array_ls[jk, 2]\n",
    "                    else:\n",
    "                        array_t[jk] = ' ', methods_name[remainder], array_ls[jk,\n",
    "                                                                             0], array_ls[jk, 1], array_ls[jk, 2]\n",
    "        if len(Data_names) > 0:\n",
    "            np.savetxt(f\"cov_wid_score_{regr_name}_solar.txt\", array_t, fmt=(\n",
    "                '%s', '%s', '%1.2f', '%1.2f', '%.2e'), delimiter=' & ', newline=' \\\\\\\\\\n', comments='')\n",
    "        else:\n",
    "            np.savetxt(f\"cov_wid_score_{regr_name}.txt\", array_t, fmt=(\n",
    "                '%s', '%s', '%1.2f', '%1.2f', '%.2e'), delimiter=' & ', newline=' \\\\\\\\\\n', comments='')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db51b8f1-6044-4b69-9ca0-29ecefaac974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_tseries(data_name, which):\n",
    "    data1 = pd.read_csv(f'Results/{data_name}_many_alpha_new_tseries.csv')\n",
    "    data2 = pd.read_csv(f'Results/{data_name}_many_alpha_new{which}.csv')\n",
    "    data1 = pd.concat((data1, data2))\n",
    "    data1.reset_index(inplace=True)\n",
    "    print(data1.shape)\n",
    "    data1.to_csv(f'Results/{data_name}_many_alpha_new{which}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b45e173-147b-4754-8be8-6d6937e2a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably I do not need this function\n",
    "def missing_data(data, missing_frac, update=False):\n",
    "    n = len(data)\n",
    "    idx = np.random.choice(n, size=int(missing_frac * n), replace=False)\n",
    "    if update:\n",
    "        data = np.delete(data, idx, 0)\n",
    "    idx = idx.tolist()\n",
    "    return (data, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d291f636-3bc2-4261-9bbe-768b16b412fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this seems to be for one dimensional time series data, probably I do not need it\n",
    "def restructure_X_t(darray):\n",
    "    '''\n",
    "    For each row i after the first row, take i-1 last entries of the first row and then impute the rest\n",
    "    Imputation is just generating random N(Y_train_mean, Y_train_std), where\n",
    "    Y_train is the first row.\n",
    "    '''\n",
    "    s = darray.shape[1]\n",
    "    copy = np.copy(darray)\n",
    "    for i in range(1, min(s, darray.shape[0])):\n",
    "        copy[i, :s - i] = copy[0, i:]\n",
    "        imputed_val = np.abs(np.random.normal(loc=np.mean(\n",
    "            copy[0]), scale=np.std(copy[0]), size=i))\n",
    "        copy[i, s - i:] = imputed_val\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9452dd1-b4dd-480d-85ab-8ca742916d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_preprocess(data, response_name='DHI', suffix=''):\n",
    "    '''Extract non-zero hours and also hours between 10AM-2PM (where radiation is high) '''\n",
    "    max_recorder = pd.DataFrame(np.zeros(24), index=range(0, 24))\n",
    "    for i in range(0, 24):\n",
    "        # Check at what times max recording is 0 (meaning no recording yet)\n",
    "        # 12:00 AM every day. for every later hour, + i \\in \\{1,...,23\\}\n",
    "        time = np.arange(365) * 24 + i\n",
    "        max_record = np.max(data[response_name][time])\n",
    "        max_recorder.iloc[i] = max_record\n",
    "    # Drop these non-zero things\n",
    "    data_sub = data.copy()\n",
    "    to_be_droped = np.where(max_recorder == 0)[0]\n",
    "    print(to_be_droped)\n",
    "    drop_idx = []\n",
    "    if len(to_be_droped) > 0:\n",
    "        for i in to_be_droped:\n",
    "            drop_idx.append(np.arange(365) * 24 + i)\n",
    "        drop_idx = np.hstack(drop_idx)\n",
    "        data_sub.drop(drop_idx, inplace=True)\n",
    "    else:\n",
    "        data_sub = []\n",
    "    # Create near_noon data between 10AM-2PM\n",
    "    if suffix == '':\n",
    "        to_be_included = np.array([10, 11, 12, 13, 14])\n",
    "    if suffix == '_8_9_15_16_17':\n",
    "        to_be_included = np.array([8, 9, 15, 16, 17])\n",
    "    if suffix == '_10_14':\n",
    "        to_be_included = np.array([10, 11, 12, 13, 14])\n",
    "    to_be_droped = np.delete(np.arange(24), to_be_included)\n",
    "    data_near_noon = data.copy()\n",
    "    drop_idx = []\n",
    "    for i in to_be_droped:\n",
    "        drop_idx.append(np.arange(365) * 24 + i)\n",
    "    drop_idx = np.hstack(drop_idx)\n",
    "    data_near_noon.drop(drop_idx, inplace=True)\n",
    "    return [data_sub, data_near_noon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5f52a5-fba0-4e0e-843a-4c9f7247ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_transform_s_beyond_1(sub, cities, current_city, one_dim, missing, miss_frac=0.25):\n",
    "    '''Overall, include ALL other cities' data in the CURRENT city being considered.\n",
    "       1. Check what data is used (full, sub, or near-noon), need sub, but it is now suppressed.\n",
    "       # NOTE, 1 is suppressed for now, since we are uncertain whether sub or near-noon is needed for Californian results\n",
    "       2. If missing, process these training and testing data before transform\n",
    "       -->> Current city and neighbors are assumed to have DIFFERENT missing fractions.\n",
    "       3. Then, if one_dim, transform data (include past), but since s>1, apply *restructure_X_t* to s rows a time'''\n",
    "    big_X_train = []\n",
    "    big_X_predict = []\n",
    "    big_Y_train = []\n",
    "    big_Y_predict = []\n",
    "    stride_ls = []\n",
    "    for city in cities:\n",
    "        print(city)\n",
    "        # Start 1\n",
    "        if 'Solar_Atl' in city:\n",
    "            data_full = eval(f'dataSolar_Atl')  # Pandas DataFrame\n",
    "            suffix = city[9:]\n",
    "            _, data = further_preprocess(data_full, suffix=suffix)\n",
    "            if suffix == '_10_14':\n",
    "                stride = 5\n",
    "            if suffix == '_8_9_15_16_17':\n",
    "                stride = 5\n",
    "        else:\n",
    "            data_full = eval(f'data{city}')  # Pandas DataFrame\n",
    "            if city == 'Wind_Austin':\n",
    "                data_sub, data_near_noon = further_preprocess(\n",
    "                    data_full, response_name='MWH')\n",
    "            else:\n",
    "                data_sub, data_near_noon = further_preprocess(data_full)\n",
    "            if sub == 0:\n",
    "                data = data_full\n",
    "                stride = 24\n",
    "            elif sub == 1:\n",
    "                data = data_sub\n",
    "                stride = int(len(data) / 365)\n",
    "            else:\n",
    "                data = data_near_noon\n",
    "                stride = 5\n",
    "        train_size = 92 * stride\n",
    "        col_name = 'MWH' if city == 'Wind_Austin' else 'DHI'\n",
    "        data_x = data.loc[:, data.columns != col_name]\n",
    "        data_y = data[col_name]\n",
    "        data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "        data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "        X_train = data_x_numpy[:train_size, :]\n",
    "        X_predict = data_x_numpy[train_size:, :]\n",
    "        Y_train_del = data_y_numpy[:train_size]\n",
    "        Y_predict_del = data_y_numpy[train_size:]\n",
    "        # Finish 1\n",
    "        # Start 2\n",
    "        if missing:\n",
    "            X_train, miss_train_idx = missing_data(\n",
    "                X_train, missing_frac=miss_frac, update=True)\n",
    "            Y_train_del = np.delete(Y_train_del, miss_train_idx)\n",
    "            Y_predict_del, miss_test_idx = missing_data(\n",
    "                Y_predict_del, missing_frac=miss_frac, update=False)\n",
    "            if city == current_city:\n",
    "                # Need an additional Y_truth\n",
    "                Y_train = Y_train_del\n",
    "                Y_predict = Y_predict_del.copy()\n",
    "                true_miss_text_idx = miss_test_idx\n",
    "            Y_predict_del[miss_test_idx] = np.abs(np.random.normal(loc=np.mean(\n",
    "                Y_train_del), scale=np.std(Y_train_del), size=len(miss_test_idx)))\n",
    "\n",
    "        else:\n",
    "            true_miss_text_idx = []\n",
    "            if city == current_city:\n",
    "                Y_train = Y_train_del\n",
    "                Y_predict = Y_predict_del\n",
    "        # Finish 2\n",
    "        # Start 3\n",
    "        if one_dim:\n",
    "            X_train, X_predict, Y_train_del, Y_predict_del = util.one_dimen_transform(\n",
    "                Y_train_del, Y_predict_del, d=min(stride, 24))  # Note: this handles 'no_slide (stride=infty)' case\n",
    "            j = 0\n",
    "            for k in range(len(X_predict) // stride + 1):\n",
    "                X_predict[j * k:min((j + 1) * k, len(X_predict))\n",
    "                          ] = restructure_X_t(X_predict[j * k:min((j + 1) * k, len(X_predict))])\n",
    "                j += 1\n",
    "            big_X_train.append(X_train)\n",
    "            big_X_predict.append(X_predict)\n",
    "            if city == current_city:\n",
    "                Y_train = Y_train_del\n",
    "                Y_predict = Y_predict_del\n",
    "        else:\n",
    "            big_X_train.append(X_train)\n",
    "            big_X_predict.append(X_predict)\n",
    "        # Finish 3\n",
    "    X_train = np.hstack(big_X_train)\n",
    "    X_predict = np.hstack(big_X_predict)\n",
    "    return([X_train, X_predict, Y_train, Y_predict, true_miss_text_idx, stride])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34d9af82-1595-4640-af0f-0c0c8932afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_together(Data_name, sub, no_slide, missing, miss_frac=0.25, one_dim=False):\n",
    "    methods = ['Ensemble']\n",
    "    train_days = 92\n",
    "    itrial = 1\n",
    "    results_ls = {}\n",
    "    alpha = 0.1\n",
    "    B = np.random.binomial(100, np.exp(-1))  # number of bootstrap samples\n",
    "    if 'Solar_Atl' in Data_name:\n",
    "        Data_name = ['Solar_Atl_8_9_15_16_17', 'Solar_Atl_10_14']\n",
    "    for data_name in Data_name:\n",
    "        np.random.seed(98765)\n",
    "        # Note, this is necessary because a model may \"remember the past\"\n",
    "        nnet = util.keras_mod()\n",
    "        if 'Solar_Atl' in data_name:\n",
    "            X_train, X_predict, Y_train, Y_predict, miss_test_idx, stride = big_transform_s_beyond_1(\n",
    "                sub, [data_name], data_name, one_dim, missing)\n",
    "        else:\n",
    "            X_train, X_predict, Y_train, Y_predict, miss_test_idx, stride = big_transform_s_beyond_1(\n",
    "                sub, Data_name, data_name, one_dim, missing)\n",
    "        train_size = 92 * stride\n",
    "        print(f'At train_size={train_size}')\n",
    "        print(f'For {data_name}')\n",
    "        if no_slide:\n",
    "            stride = int((365 - 92) * stride)  # No slide at all\n",
    "        print(stride)\n",
    "        nnet = util.keras_mod()\n",
    "        min_alpha = 0.0001\n",
    "        max_alpha = 10\n",
    "        ridge_cv = RidgeCV(alphas=np.linspace(min_alpha, max_alpha, 10))\n",
    "        random_forest = RandomForestRegressor(n_estimators=10, criterion='mse',\n",
    "                                              bootstrap=False, max_depth=2, n_jobs=-1)\n",
    "        ridge_results = EnbPI.prediction_interval(\n",
    "            ridge_cv,  X_train, X_predict, Y_train, Y_predict)\n",
    "        ridge_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "        rf_results = EnbPI.prediction_interval(\n",
    "            random_forest,  X_train, X_predict, Y_train, Y_predict)\n",
    "        rf_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "        nn_results = EnbPI.prediction_interval(\n",
    "            nnet,  X_train, X_predict, Y_train, Y_predict)\n",
    "        nn_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "        # For CP Methods\n",
    "        print(f'regressor is {ridge_cv.__class__.__name__}')\n",
    "        result_ridge = ridge_results.run_experiments(\n",
    "            alpha, stride, data_name, itrial, methods=methods, get_plots=True)\n",
    "        result_ridge[0]['center'] = ridge_results.Ensemble_pred_interval_centers\n",
    "        print(f'regressor is {random_forest.__class__.__name__}')\n",
    "        result_rf = rf_results.run_experiments(\n",
    "            alpha, stride, data_name, itrial, methods=methods, get_plots=True)\n",
    "        result_rf[0]['center'] = rf_results.Ensemble_pred_interval_centers\n",
    "        print(f'regressor is {nnet.name}')\n",
    "        result_nn = nn_results.run_experiments(\n",
    "            alpha, stride, data_name, itrial, methods=methods, get_plots=True)\n",
    "        result_nn[0]['center'] = nn_results.Ensemble_pred_interval_centers\n",
    "        results_ls[data_name] = [result_ridge, result_rf, result_nn, stride, Y_train, Y_predict, ridge_results.Ensemble_online_resid[:train_days],\n",
    "                                 rf_results.Ensemble_online_resid[:train_days], nn_results.Ensemble_online_resid[:train_days]]\n",
    "    return results_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de2d8e45-20cb-4059-a8ea-5a26706c48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_helper(results_ls):\n",
    "    names = list(results_ls.keys())\n",
    "    result_ridge_ls = []\n",
    "    result_rf_ls = []\n",
    "    result_nn_ls = []\n",
    "    Y_train_ls = []\n",
    "    Y_predict_ls = []\n",
    "    stride_ls = []\n",
    "    ridge_resid_ls = []\n",
    "    rf_resid_ls = []\n",
    "    nn_resid_ls = []\n",
    "    for data_name in names:\n",
    "        result_ridge, result_rf, result_nn, stride, Y_train, Y_predict, ridge_resid, rf_resid, nn_resid = results_ls[\n",
    "            data_name]\n",
    "        result_ridge_ls.append(result_ridge[0])\n",
    "        ridge_resid_ls.append(ridge_resid)\n",
    "        result_rf_ls.append(result_rf[0])\n",
    "        rf_resid_ls.append(rf_resid)\n",
    "        result_nn_ls.append(result_nn[0])\n",
    "        nn_resid_ls.append(nn_resid)\n",
    "        Y_train_ls.append(Y_train)\n",
    "        Y_predict_ls.append(Y_predict)\n",
    "        stride_ls.append(stride)\n",
    "    results_dict = {'Ridge': [result_ridge_ls, ridge_resid_ls], 'RF': [\n",
    "        result_rf_ls, rf_resid_ls], 'NN': [result_nn_ls, nn_resid_ls]}\n",
    "    return [results_dict, Y_train_ls, Y_predict_ls, stride_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a53c5d2-d61a-40c2-816b-1b97f2a776b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis = pd.read_csv('./Data/fully_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6643db7-b9dc-4fd8-ab9a-4aac281c7c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pat_id', 'HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'Glucose',\n",
       "       'Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS',\n",
       "       'SepsisLabel', 'hours2sepsis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7787f15-8bea-4b1d-a4a4-e89cfff5c18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.045029</td>\n",
       "      <td>97.403481</td>\n",
       "      <td>36.708390</td>\n",
       "      <td>124.795742</td>\n",
       "      <td>85.718264</td>\n",
       "      <td>67.642071</td>\n",
       "      <td>18.833348</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p119046</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.644472</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p119046</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>36.662368</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p119046</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>36.651706</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_id         HR      O2Sat       Temp         SBP         MAP  \\\n",
       "0  p119046  84.045029  97.403481  36.708390  124.795742   85.718264   \n",
       "1  p119046  85.000000  98.000000  36.644472  168.000000  123.000000   \n",
       "2  p119046  81.000000  97.500000  36.662368  163.500000  119.000000   \n",
       "3  p119046  83.000000  98.000000  36.500000  158.000000  117.000000   \n",
       "4  p119046  84.000000  97.000000  36.651706  154.000000  113.000000   \n",
       "\n",
       "         DBP       Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0  67.642071  18.833348  131.356454  60.0       1    1.0    0.0        -7.79   \n",
       "1  94.000000  22.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "2  90.000000  24.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "3  89.000000  22.500000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "4  85.000000  20.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "\n",
       "   ICULOS  SepsisLabel  hours2sepsis  \n",
       "0     1.0            0           500  \n",
       "1     2.0            0           500  \n",
       "2     3.0            0           500  \n",
       "3     4.0            0           500  \n",
       "4     5.0            0           500  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bee4c6-07f5-4f1f-867b-b0da154a99e7",
   "metadata": {},
   "source": [
    "### Algorithm description from the paper (EnbPI) \n",
    " \n",
    "On a high-level, EnbPI has a training phase and a prediction phase. In the training phase, EnbPI first fit a fixed number (i.e., B) \n",
    "of bootstrap estimators from  subsets of the training data (i.e., train_size 20%). Then, it aggregates predictions from these \n",
    "bootstrap estimators on the training data in an efficient leave-one-put (LOO) fashion, resulting in both LOO estimators \n",
    "and LOO residuals.\n",
    "\n",
    "In the prediction phase, EnbPI aggregates predictions from LOO predictors (e.g., RidgeCV) on each test datum\n",
    "to compute the center of the prediction interval.\n",
    "Then it builds the prediction intervel using the past LOO residuals, where the interval width is also\n",
    "optimized through a simple one-dimensional line search. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b5d8757-fc05-42de-bf8a-c37e58859a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============Read data and initialize parameters\n",
    "result_type = 'sepsis'\n",
    "response_ls = {'sepsis': 'hours2sepsis', 'Solar_Atl': 'DHI', 'Palo_Alto': 'DHI', 'Wind_Austin': 'MWH',\n",
    "               'green_house': 15, 'appliances': 'Appliances', 'Beijing_air': 'PM2.5', }\n",
    "##!!! Focus on this one\n",
    "if result_type == 'sepsis':\n",
    "    # sepsis\n",
    "    datasepsis = pd.read_csv('./Data/fully_imputed.csv')\n",
    "    datasepsis.drop(columns=['pat_id'], inplace = True, axis =1)\n",
    "    Data_name = ['sepsis']\n",
    "    CA_energy_data = False\n",
    "elif result_type == 'Fig3':\n",
    "    # Figure 3\n",
    "    max_data_size = 10000\n",
    "    dataSolar_Atl = util.read_data(3, './Data/Solar_Atl_data.csv', max_data_size)\n",
    "    # Data_name = ['Solar_Atl']\n",
    "    Data_name = ['Solar_Atl']\n",
    "    CA_energy_data = False\n",
    "elif result_type == 'AppendixB3':\n",
    "    # Results in Appendix B.3\n",
    "    CA_cities = ['Fremont', 'Milpitas', 'Mountain_View', 'North_San_Jose',\n",
    "                 'Palo_Alto', 'Redwood_City', 'San_Mateo', 'Santa_Clara',\n",
    "                 'Sunnyvale']\n",
    "    for city in CA_cities:\n",
    "        globals()['data%s' % city] = util.read_CA_data(f'Data/{city}_data.csv')\n",
    "    Data_name = ['Palo_Alto']\n",
    "    CA_energy_data = True\n",
    "else:\n",
    "    # Results in Appendix B.4\n",
    "    datagreen_house = util.read_data(\n",
    "        0, './Data/green_house_data.csv', max_data_size)\n",
    "    dataappliances_data = util.read_data(\n",
    "        1, './Data/appliances_data.csv', max_data_size)\n",
    "    dataBeijing_air = util.read_data(\n",
    "        2, './Data/Beijing_air_Tiantan_data.csv', max_data_size)\n",
    "    Data_name = ['green_house', 'appliances', 'Beijing_air']\n",
    "min_alpha = 0.0001\n",
    "max_alpha = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95f0ea14-db26-4f93-9b97-a9a0f2083269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.045029</td>\n",
       "      <td>97.403481</td>\n",
       "      <td>36.708390</td>\n",
       "      <td>124.795742</td>\n",
       "      <td>85.718264</td>\n",
       "      <td>67.642071</td>\n",
       "      <td>18.833348</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.644472</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>36.662368</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>36.651706</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          HR      O2Sat       Temp         SBP         MAP        DBP  \\\n",
       "0  84.045029  97.403481  36.708390  124.795742   85.718264  67.642071   \n",
       "1  85.000000  98.000000  36.644472  168.000000  123.000000  94.000000   \n",
       "2  81.000000  97.500000  36.662368  163.500000  119.000000  90.000000   \n",
       "3  83.000000  98.000000  36.500000  158.000000  117.000000  89.000000   \n",
       "4  84.000000  97.000000  36.651706  154.000000  113.000000  85.000000   \n",
       "\n",
       "        Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  ICULOS  \\\n",
       "0  18.833348  131.356454  60.0       1    1.0    0.0        -7.79     1.0   \n",
       "1  22.000000   98.000000  60.0       1    1.0    0.0        -7.79     2.0   \n",
       "2  24.000000   98.000000  60.0       1    1.0    0.0        -7.79     3.0   \n",
       "3  22.500000   98.000000  60.0       1    1.0    0.0        -7.79     4.0   \n",
       "4  20.000000   98.000000  60.0       1    1.0    0.0        -7.79     5.0   \n",
       "\n",
       "   SepsisLabel  hours2sepsis  \n",
       "0            0           500  \n",
       "1            0           500  \n",
       "2            0           500  \n",
       "3            0           500  \n",
       "4            0           500  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasepsis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8c984ae-d804-403b-81b5-2597672071cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR              0.0\n",
       "O2Sat           0.0\n",
       "Temp            0.0\n",
       "SBP             0.0\n",
       "MAP             0.0\n",
       "DBP             0.0\n",
       "Resp            0.0\n",
       "Glucose         0.0\n",
       "Age             0.0\n",
       "Gender          0.0\n",
       "Unit1           0.0\n",
       "Unit2           0.0\n",
       "HospAdmTime     0.0\n",
       "ICULOS          0.0\n",
       "SepsisLabel     0.0\n",
       "hours2sepsis    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasepsis.isnull().sum()/len(sepsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90617a-52ee-48b3-a523-dc062bc73c7b",
   "metadata": {},
   "source": [
    "### Training the predictors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c80f182a-6a95-4e25-88c8-c296a95421e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV:\n",
    "# Ridge regression with built-in cross-validation.\n",
    "# By default, it performs efficient Leave-One-Out Cross-Validation.\n",
    "## min|xw+y|^2 + alpha*|w|^2\n",
    "## get an Array of alpha values to try\n",
    "ridge_cv = RidgeCV(alphas=np.linspace(min_alpha, max_alpha, 10))\n",
    "random_forest = RandomForestRegressor(n_estimators=10, criterion='mse',\n",
    "                                      bootstrap=False, max_depth=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d94ac6b-0812-4f01-b908-9ea4f2cc3d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'PI_class_EnbPI_journal' from '/Users/anniezhou/Desktop/EnbPI_new_c/PI_class_EnbPI_journal.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_ls = np.linspace(0.05, 0.25, 5) \n",
    "importlib.reload(sys.modules['PI_class_EnbPI_journal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab03fce-c12e-44a9-a9cf-5528550a35d5",
   "metadata": {},
   "source": [
    "After walking through the source code in _utils_EnbPI_journal.py_, I think the code below might not be applicable for our case. Because we cannot treat our data as a complete \n",
    "time series data, though we can have the data of each patient as a time series data. Anyway, I think I need to modify this part and change the way models are trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3279f0ec-1174-41c3-a89c-556a2a9c5b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At trial # 0 and alpha=0.05\n",
      "For sepsis\n",
      "Not using Conformal Prediction Methods\n",
      "Running ARIMA(10,1,10)\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966833391285741\n",
      "Average Width is 100.25269638339766\n",
      "Running ExpSmoothing\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966793270388102\n",
      "Average Width is 106.19150399733807\n",
      "Running DynamicFactor\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966833391285741\n",
      "Average Width is 100.8718411162998\n",
      "At trial # 0 and alpha=0.1\n",
      "For sepsis\n",
      "Not using Conformal Prediction Methods\n",
      "Running ARIMA(10,1,10)\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966753149490465\n",
      "Average Width is 84.13471500426272\n",
      "Running ExpSmoothing\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966713028592826\n",
      "Average Width is 89.1187194658798\n",
      "Running DynamicFactor\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966766523123011\n",
      "Average Width is 84.65431764367555\n",
      "At trial # 0 and alpha=0.15000000000000002\n",
      "For sepsis\n",
      "Not using Conformal Prediction Methods\n",
      "Running ARIMA(10,1,10)\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966726402225372\n",
      "Average Width is 73.63243030417497\n",
      "Running ExpSmoothing\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966686281327735\n",
      "Average Width is 77.99429640352717\n",
      "Running DynamicFactor\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966726402225372\n",
      "Average Width is 74.08717250102585\n",
      "At trial # 0 and alpha=0.2\n",
      "For sepsis\n",
      "Not using Conformal Prediction Methods\n",
      "Running ARIMA(10,1,10)\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.996669965496028\n",
      "Average Width is 65.5517147323301\n",
      "Running ExpSmoothing\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966606039532457\n",
      "Average Width is 69.43489230862612\n",
      "Running DynamicFactor\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966686281327735\n",
      "Average Width is 65.9565517130134\n",
      "At trial # 0 and alpha=0.25\n",
      "For sepsis\n",
      "Not using Conformal Prediction Methods\n",
      "Running ARIMA(10,1,10)\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9958488244576992\n",
      "Average Width is 58.840686908197966\n",
      "Running ExpSmoothing\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966592665899912\n",
      "Average Width is 62.32631404867424\n",
      "Running DynamicFactor\n",
      "training\n",
      "training done\n",
      "Average Coverage is 0.9966672907695188\n",
      "Average Width is 59.20407764673955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First run time-series methods\n",
    "# focus on Data_name = ['Solar_Atl']\n",
    "starttime = time.time()\n",
    "for data_name in Data_name:\n",
    "    # e.g., data_name = 'Solar_Atl'\n",
    "    # one_dim = True\n",
    "    itrial = 0\n",
    "    # The eval() method parses the expression passed to this method \n",
    "    # and runs python expression (code) within the program.\n",
    "    # The string itself can be formatted in much the same way that you would with str.format(). \n",
    "    # F-strings provide a concise and convenient way to embed python expressions \n",
    "    # inside string literals for formatting. \n",
    "    data = eval(f'data{data_name}')  # Pandas DataFrame, ????????\n",
    "# response_ls = {'Solar_Atl': 'DHI', 'Palo_Alto': 'DHI', 'Wind_Austin': 'MWH',\n",
    "#                'green_house': 15, 'appliances': 'Appliances', 'Beijing_air': 'PM2.5', }\n",
    "# response_ls stores the label names of all datasets\n",
    "    data_x = data.loc[:, data.columns != response_ls[data_name]]\n",
    "    data_y = data[response_ls[data_name]]\n",
    "    data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "    data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "    total_data_points = data_x_numpy.shape[0]\n",
    "    train_size = int(0.2 * total_data_points)\n",
    "    \n",
    "    results_ts = pd.DataFrame(columns=['itrial', 'dataname',\n",
    "                                       'method', 'alpha', 'coverage', 'width'])\n",
    "    np.random.seed(98765 + itrial)\n",
    "    # train the predictors with different alphas\n",
    "    # alpha_ls = np.linspace(0.05, 0.25, 5)\n",
    "    for alpha in alpha_ls:\n",
    "        print(f'At trial # {itrial} and alpha={alpha}')\n",
    "        # e.g., data_name = 'Solar_Atl'\n",
    "        print(f'For {data_name}')\n",
    "        # if CA_energ\n",
    "        # y_data:\n",
    "        #     X_train, X_predict, Y_train, Y_predict = big_transform(\n",
    "        #         Data_name, data_name, one_dim, train_size)\n",
    "        #     d = 20\n",
    "        # ## e.g., data_name = 'Solar_Atl'\n",
    "        # else:\n",
    "            # X_train = data_x_numpy[:train_size, :]\n",
    "            # X_predict = data_x_numpy[train_size:, :]\n",
    "            # Y_train = data_y_numpy[:train_size]\n",
    "            # Y_predict = data_y_numpy[train_size:]\n",
    "        X_train = data_x_numpy[:train_size, :]\n",
    "        X_predict = data_x_numpy[train_size:, :]\n",
    "        Y_train = data_y_numpy[:train_size]\n",
    "        Y_predict = data_y_numpy[train_size:]\n",
    "        # import PI_class_EnbPI_journal as EnbPI\n",
    "        ridge_results = EnbPI.prediction_interval(\n",
    "            ridge_cv,  X_train, X_predict, Y_train, Y_predict)\n",
    "        # if stride ==1, we do not need to predict multiple steps ahead (May 2nd, 2023)\n",
    "        stride = 1 # not sure????? added by Annie Zhou Feb 15th, 2023\n",
    "        # For ARIMA and other time-series methods, only run once\n",
    "        result_ts = ridge_results.run_experiments(\n",
    "            alpha, stride, data_name, itrial, none_CP=True)\n",
    "        result_ts.rename(columns={'train_size': 'alpha'}, inplace=True)\n",
    "        # if CA_energy_data:\n",
    "        #     result_ts['alpha'].replace(\n",
    "        #         train_size - d, alpha, inplace=True)\n",
    "        # ## e.g., data_name = 'Solar_Atl'\n",
    "        # else:\n",
    "        #     #  train_size = int(0.2 * total_data_points)\n",
    "        #     result_ts['alpha'].replace(\n",
    "        #         train_size, alpha, inplace=True)\n",
    "        result_ts['alpha'].replace(train_size, alpha, inplace=True)\n",
    "        results_ts = bsu ecpd.concat([results_ts, result_ts])\n",
    "        results_ts.to_csv(\n",
    "            f'Results/{data_name}_many_alpha_new_tseries.csv', index=False)\n",
    "print('Time spent to train the models (non-CP): %s' (starttime-time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd91c3c-aac3-48b1-8583-d22d5eff830b",
   "metadata": {},
   "source": [
    "Getting the conformal intervals. Then run conformal-related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a6958-e0ab-4181-b45c-33f91699ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First run time-series methods\n",
    "# focus on Data_name = ['Solar_Atl']\n",
    "starttime = time.time()\n",
    "for data_name in Data_name:\n",
    "    # e.g., data_name = 'Solar_Atl'\n",
    "    # one_dim = True\n",
    "    itrial = 0\n",
    "    # The eval() method parses the expression passed to this method \n",
    "    # and runs python expression (code) within the program.\n",
    "    # The string itself can be formatted in much the same way that you would with str.format(). \n",
    "    # F-strings provide a concise and convenient way to embed python expressions \n",
    "    # inside string literals for formatting. \n",
    "    data = eval(f'data{data_name}')  # Pandas DataFrame, ????????\n",
    "# response_ls = {'Solar_Atl': 'DHI', 'Palo_Alto': 'DHI', 'Wind_Austin': 'MWH',\n",
    "#                'green_house': 15, 'appliances': 'Appliances', 'Beijing_air': 'PM2.5', }\n",
    "# response_ls stores the label names of all datasets\n",
    "    data_x = data.loc[:, data.columns != response_ls[data_name]]\n",
    "    data_y = data[response_ls[data_name]]\n",
    "    data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "    data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "    total_data_points = data_x_numpy.shape[0]\n",
    "    train_size = int(0.2 * total_data_points)\n",
    "    \n",
    "    results_ts = pd.DataFrame(columns=['itrial', 'dataname',\n",
    "                                       'method', 'alpha', 'coverage', 'width'])\n",
    "    np.random.seed(98765 + itrial)\n",
    "    # train the predictors with different alphas\n",
    "    # alpha_ls = np.linspace(0.05, 0.25, 5)\n",
    "    for alpha in alpha_ls:\n",
    "        print(f'At trial # {itrial} and alpha={alpha}')\n",
    "        # e.g., data_name = 'Solar_Atl'\n",
    "        print(f'For {data_name}')\n",
    "        # if CA_energ\n",
    "        # y_data:\n",
    "        #     X_train, X_predict, Y_train, Y_predict = big_transform(\n",
    "        #         Data_name, data_name, one_dim, train_size)\n",
    "        #     d = 20\n",
    "        # ## e.g., data_name = 'Solar_Atl'\n",
    "        # else:\n",
    "            # X_train = data_x_numpy[:train_size, :]\n",
    "            # X_predict = data_x_numpy[train_size:, :]\n",
    "            # Y_train = data_y_numpy[:train_size]\n",
    "            # Y_predict = data_y_numpy[train_size:]\n",
    "        X_train = data_x_numpy[:train_size, :]\n",
    "        X_predict = data_x_numpy[train_size:, :]\n",
    "        Y_train = data_y_numpy[:train_size]\n",
    "        Y_predict = data_y_numpy[train_size:]\n",
    "        # import PI_class_EnbPI_journal as EnbPI\n",
    "        ridge_results = EnbPI.prediction_interval(\n",
    "            ridge_cv,  X_train, X_predict, Y_train, Y_predict)\n",
    "        # if stride ==1, we do not need to predict multiple steps ahead (May 2nd, 2023)\n",
    "        stride = 1 # not sure????? added by Annie Zhou Feb 15th, 2023\n",
    "        # For ARIMA and other time-series methods, only run once\n",
    "        \n",
    "        result_ts = ridge_results.run_experiments(\n",
    "            alpha, stride, data_name, itrial, none_CP=True)\n",
    "        result_ts.rename(columns={'train_size': 'alpha'}, inplace=True)\n",
    "        # if CA_energy_data:\n",
    "        #     result_ts['alpha'].replace(\n",
    "        #         train_size - d, alpha, inplace=True)\n",
    "        # ## e.g., data_name = 'Solar_Atl'\n",
    "        # else:\n",
    "        #     #  train_size = int(0.2 * total_data_points)\n",
    "        #     result_ts['alpha'].replace(\n",
    "        #         train_size, alpha, inplace=True)\n",
    "        result_ts['alpha'].replace(train_size, alpha, inplace=True)\n",
    "        results_ts = bsu ecpd.concat([results_ts, result_ts])\n",
    "        results_ts.to_csv(\n",
    "            f'Results/{data_name}_many_alpha_new_tseries.csv', index=False)\n",
    "print('Time spent to train the models (non-CP): %s' (starttime-time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
