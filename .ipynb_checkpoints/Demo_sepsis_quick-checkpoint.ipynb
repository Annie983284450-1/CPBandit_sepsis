{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from Demo_code.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xysGqjV0p5aE"
   },
   "source": [
    "This file demonstrate the solar experiment shown in section 6.3, where we compare EnbPI, SPCI, AdaptiveCI, and Nex-CP WLS\n",
    "\n",
    "It is current written to be executed in google colaboratory.\n",
    "\n",
    "Please place the following files in the same directory as this `.ipynb`, which are a part of this Github repository\n",
    "\n",
    "```\n",
    "  utils_quick.py\n",
    "\n",
    "  utils_latest.py\n",
    "\n",
    "  utils_EnbPI.py\n",
    "\n",
    "  PI_class_EnbPI.py\n",
    "\n",
    "  Data/Solar_Atl_data.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DQL2E4phhx9P"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34663,
     "status": "ok",
     "timestamp": 1683155293498,
     "user": {
      "displayName": "anni zhou",
      "userId": "09983559370256030799"
     },
     "user_tz": 360
    },
    "id": "CXt0Szm9q_Xx",
    "outputId": "f7c8e319-5c7e-49eb-e226-8fcd8a190b3c"
   },
   "outputs": [],
   "source": [
    "# !pip install pickle5\n",
    "# !pip install --upgrade --no-deps statsmodels\n",
    "# !pip install scikit-garden\n",
    "# !pip install scikit-learn==0.22.2 # Needed for scikit-garden to work properly. Restart runtime is NOT needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KZWdnDfi3oBu",
    "outputId": "58f81cd7-c4f8-4df1-e2d3-e9b17825f1c5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# on windows:\n",
    "# !pip install scikit-garden==0.22.1 \n",
    "# on mac:\n",
    "# !pip install scikit-garden==0.22.2\n",
    "import skgarden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "y4DWIPYmcqY5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# path = '/content/drive/MyDrive/EnbPI' # You are free to replace this with where you place this file on drive\n",
    "# sys.path.insert(0,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8bloW_QoiZNE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/EnbPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rT3L4tHA3oBu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MPS acceleration is available on MacOS 12.3+\n",
    "# !pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "eWSzk0dNQf8W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import importlib as ipb\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "import math\n",
    "import time as time\n",
    "import utils_quick as util\n",
    "import utils_latest\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from skgarden import RandomForestQuantileRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjuucWAivJOC"
   },
   "source": [
    "# Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2blvGaelQXAO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Main Class ####\n",
    "class prediction_interval_with_SPCI():\n",
    "    '''\n",
    "        Create prediction intervals assuming Y_t = f(X_t) + \\sigma(X_t)\\eps_t\n",
    "        Currently, assume the regression function is by default MLP implemented with PyTorch, as it needs to estimate BOTH f(X_t) and \\sigma(X_t), where the latter is impossible to estimate using scikit-learn modules\n",
    "\n",
    "        Most things carry out, except that we need to have different estimators for f and \\sigma.\n",
    "\n",
    "        fit_func = None: use MLP above\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X_train, X_predict, Y_train, Y_predict, fit_func=None):\n",
    "        self.regressor = fit_func\n",
    "        self.X_train = X_train\n",
    "        self.X_predict = X_predict\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_predict = Y_predict\n",
    "        # Predicted training data centers by EnbPI\n",
    "        self.Ensemble_train_interval_centers = []\n",
    "        self.Ensemble_train_interval_sigma = []\n",
    "        # Predicted test data centers by EnbPI\n",
    "        self.Ensemble_pred_interval_centers = []\n",
    "        self.Ensemble_pred_interval_sigma = []\n",
    "        self.Ensemble_online_resid = []  # LOO scores\n",
    "        self.beta_hat_bins = []\n",
    "\n",
    "    def fit_bootstrap_models_online(self, B, miss_test_idx=[], fit_sigmaX=True):\n",
    "        '''\n",
    "          Train B bootstrap estimators from subsets of (X_train, Y_train), compute aggregated predictors, and compute the residuals\n",
    "          fit_sigmaX: If False, just avoid predicting \\sigma(X_t) by defaulting it to 1\n",
    "        '''\n",
    "        n, d = self.X_train.shape\n",
    "        n1 = len(self.X_predict)\n",
    "        # hold indices of training data for each f^b\n",
    "        boot_samples_idx = util.generate_bootstrap_samples(n, n, B)\n",
    "        # hold predictions from each f^b for fX and sigma&b for sigma\n",
    "        boot_predictionsFX = torch.zeros(B, n+n1).to(device)\n",
    "        boot_predictionsSigmaX = torch.ones(B, n+n1).to(device)\n",
    "        # for i^th column, it shows which f^b uses i in training (so exclude in aggregation)\n",
    "        in_boot_sample = np.zeros((B, n), dtype=bool)\n",
    "        out_sample_predictFX = torch.zeros(n, n1).to(device)\n",
    "        out_sample_predictSigmaX = torch.ones(n, n1).to(device)\n",
    "        start = time.time()\n",
    "        Xfull = torch.vstack([self.X_train, self.X_predict])\n",
    "        for b in range(B):\n",
    "            Xboot, Yboot = self.X_train[boot_samples_idx[b],\n",
    "                                        :], self.Y_train[boot_samples_idx[b], ]\n",
    "            in_boot_sample[b, boot_samples_idx[b]] = True\n",
    "            if self.regressor.__class__.__name__ == 'NoneType':\n",
    "                start1 = time.time()\n",
    "                model_f = MLP(d).to(device)\n",
    "                optimizer_f = torch.optim.Adam(model_f.parameters(), lr=1e-3)\n",
    "                if fit_sigmaX:\n",
    "                    model_sigma = MLP(d, sigma=True).to(device)\n",
    "                    optimizer_sigma = torch.optim.Adam(\n",
    "                        model_sigma.parameters(), lr=2e-3)\n",
    "                for epoch in range(300):\n",
    "                    fXhat = model_f(Xboot)\n",
    "                    sigmaXhat = torch.ones(len(fXhat)).to(device)\n",
    "                    if fit_sigmaX:\n",
    "                        sigmaXhat = model_sigma(Xboot)\n",
    "                    loss = ((Yboot-fXhat)/sigmaXhat).pow(2).mean()/2\n",
    "                    optimizer_f.zero_grad()\n",
    "                    if fit_sigmaX:\n",
    "                        optimizer_sigma.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_f.step()\n",
    "                    if fit_sigmaX:\n",
    "                        optimizer_sigma.step()\n",
    "                with torch.no_grad():\n",
    "                    boot_predictionsFX[b] = model_f(Xfull).flatten()\n",
    "                    if fit_sigmaX:\n",
    "                        boot_predictionsSigmaX[b] = model_sigma(\n",
    "                            Xfull).flatten()\n",
    "                print(\n",
    "                    f'Took {time.time()-start1} secs to finish the {b}th boostrap model')\n",
    "            else:\n",
    "                model = self.regressor\n",
    "                model.fit(Xboot, Yboot)\n",
    "                boot_predictionsFX[b] = torch.from_numpy(\n",
    "                    model.predict(Xfull).flatten()).to(device)\n",
    "                # NOTE, NO sigma estimation because these methods by deFAULT are fitting Y, but we have no observation of errors\n",
    "        print(\n",
    "            f'Finish Fitting {B} Bootstrap models, took {time.time()-start} secs.')\n",
    "        start = time.time()\n",
    "        keep = []\n",
    "        for i in range(n):\n",
    "            b_keep = np.argwhere(~(in_boot_sample[:, i])).reshape(-1)\n",
    "            pred_iFX = boot_predictionsFX[b_keep, i].mean()\n",
    "            pred_iSigmaX = boot_predictionsSigmaX[b_keep, i].mean()\n",
    "            pred_testFX = boot_predictionsFX[b_keep, n:].mean(0)\n",
    "            pred_testSigmaX = boot_predictionsSigmaX[b_keep, n:].mean(0)\n",
    "            if(len(b_keep) > 0):\n",
    "                self.Ensemble_train_interval_centers.append(pred_iFX)\n",
    "                self.Ensemble_train_interval_sigma.append(pred_iSigmaX)\n",
    "                resid_LOO = (self.Y_train[i] - pred_iFX)/pred_iSigmaX\n",
    "                out_sample_predictFX[i] = pred_testFX\n",
    "                out_sample_predictSigmaX[i] = pred_testSigmaX\n",
    "                keep = keep+[b_keep]\n",
    "            self.Ensemble_online_resid.append(resid_LOO.item())\n",
    "        sorted_out_sample_predictFX = out_sample_predictFX.mean(0)  # length n1\n",
    "        sorted_out_sample_predictSigmaX = out_sample_predictSigmaX.mean(\n",
    "            0)  # length n1\n",
    "        resid_out_sample = (\n",
    "            self.Y_predict-sorted_out_sample_predictFX)/sorted_out_sample_predictSigmaX\n",
    "        if len(miss_test_idx) > 0:\n",
    "            # Replace missing residuals with that from the immediate predecessor that is not missing, as\n",
    "            # o/w we are not assuming prediction data are missing\n",
    "            for idx in range(len(miss_test_idx)):\n",
    "                i = miss_test_idx[idx]\n",
    "                if i > 0:\n",
    "                    j = i-1\n",
    "                    while j in miss_test_idx[:idx]:\n",
    "                        j -= 1\n",
    "                    resid_out_sample[i] = resid_out_sample[j]\n",
    "\n",
    "                else:\n",
    "                    # The first Y during testing is missing, let it be the last of the training residuals\n",
    "                    # note, training data already takes out missing values, so doing is is fine\n",
    "                    resid_out_sample[0] = self.Ensemble_online_resid[-1]\n",
    "        self.Ensemble_online_resid = np.append(\n",
    "            self.Ensemble_online_resid, resid_out_sample.cpu().detach().numpy())\n",
    "        # print(f'Finish Computing LOO residuals, took {time.time()-start} secs.')\n",
    "        # print(f'Max LOO test residual is {np.max(self.Ensemble_online_resid[n:])}')\n",
    "        # print(f'Min LOO test residual is {np.min(self.Ensemble_online_resid[n:])}')\n",
    "        self.Ensemble_pred_interval_centers = sorted_out_sample_predictFX\n",
    "        self.Ensemble_pred_interval_sigma = sorted_out_sample_predictSigmaX\n",
    "\n",
    "    def compute_PIs_Ensemble_online(self, alpha, stride=1, smallT=True, past_window=100, use_quantile_regr=False, quantile_regr='RF'):\n",
    "        '''\n",
    "            smallT: if True, we would only start with the last n number of LOO residuals, rather than use the full length T ones. Used in change detection\n",
    "                NOTE: smallT can be important if time-series is very dynamic, in which case training MORE data may actaully be worse (because quantile longer)\n",
    "                HOWEVER, if fit quantile regression, set it to be FALSE because we want to have many training pts for the quantile regressor\n",
    "            use_quantile_regr: if True, we fit conditional quantile to compute the widths, rather than simply using empirical quantile\n",
    "        '''\n",
    "        n1 = len(self.X_train)\n",
    "        if smallT:\n",
    "            n1 = min(past_window, len(self.X_train))\n",
    "        # Now f^b and LOO residuals have been constructed from earlier\n",
    "        out_sample_predict = self.Ensemble_pred_interval_centers.cpu().detach().numpy()\n",
    "        out_sample_predictSigmaX = self.Ensemble_pred_interval_sigma\n",
    "        start = time.time()\n",
    "        # Matrix, where each row is a UNIQUE slice of residuals with length stride.\n",
    "        resid_strided = util.strided_app(\n",
    "            self.Ensemble_online_resid[len(self.X_train)-n1:-1], n1, stride)\n",
    "        print(f'Shape of slided residual lists is {resid_strided.shape}')\n",
    "        num_unique_resid = resid_strided.shape[0]\n",
    "        width_left = np.zeros(num_unique_resid)\n",
    "        width_right = np.zeros(num_unique_resid)\n",
    "        # # NEW, alpha becomes alpha_t. Uncomment things below if we decide to use this upgraded EnbPI\n",
    "        # alpha_t = alpha\n",
    "        # errs = []\n",
    "        # gamma = 0.005\n",
    "        # method = 'simple'  # 'simple' or 'complex'\n",
    "        # self.alphas = []\n",
    "        # NOTE: 'max_features='log2', max_depth=2' make the model \"simpler\", which improves performance in practice\n",
    "        for i in range(num_unique_resid):\n",
    "            # for p in range(stride):  # NEW for adaptive alpha\n",
    "            past_resid = resid_strided[i, :]\n",
    "            curr_SigmaX = out_sample_predictSigmaX[i].item()\n",
    "            if use_quantile_regr:\n",
    "                # New predicted conditional quntile\n",
    "                # 1. Get \"past_resid\" into an auto-regressive fashion\n",
    "                # This should be more carefully examined, b/c it depends on how long \\hat{\\eps}_t depends on the past\n",
    "                # From practice, making it small make intervals wider\n",
    "                n2 = past_window\n",
    "                residX = sliding_window_view(past_resid, window_shape=n2)\n",
    "                residY = past_resid[n2:]\n",
    "                # 2. Fit the model. Default quantile regressor is the quantile RF from\n",
    "                # scikit-garden: https://scikit-garden.github.io/\n",
    "                # NOTE, should NOT warm start, as it makes result poor, although training is longer\n",
    "                if quantile_regr == 'RF':\n",
    "                    rfqr = RandomForestQuantileRegressor(\n",
    "                        max_depth=2, random_state=0)\n",
    "                    rfqr.fit(residX[:-1], residY)\n",
    "                    # 3. Find best \\hat{\\beta} via evaluating many quantiles\n",
    "                    beta_hat_bin = util.binning_use_RF_quantile_regr(\n",
    "                        rfqr, residX[-1], alpha)\n",
    "                    width_left[i] = curr_SigmaX*rfqr.predict(\n",
    "                        residX[-1].reshape(1, -1), math.ceil(100 * beta_hat_bin))\n",
    "                    width_right[i] = curr_SigmaX*rfqr.predict(\n",
    "                        residX[-1].reshape(1, -1), math.ceil(100 * (1-alpha+beta_hat_bin)))\n",
    "                # if quantile_regr == 'LR':\n",
    "                #     start1 = time.time()\n",
    "                #     wleft, wright = util.binning_use_linear_quantile_regr(\n",
    "                #         residX, residY, alpha)\n",
    "                #     if i == 0:\n",
    "                #         print(\n",
    "                #             f'100 Linear QRegr approx. takes {100*(time.time()-start1)} secs.')\n",
    "                #     width_left[i] = curr_SigmaX*wleft\n",
    "                #     width_right[i] = curr_SigmaX*wright\n",
    "                if i % int(num_unique_resid/20) == 0:\n",
    "                    print(\n",
    "                        f'Width at test {i} is {width_right[i]-width_left[i]}')\n",
    "            else:\n",
    "                # Naive empirical quantile\n",
    "                # The number of bins will be determined INSIDE binning\n",
    "                beta_hat_bin = util.binning(past_resid, alpha)\n",
    "                # beta_hat_bin = util.binning(past_resid, alpha_t)\n",
    "                self.beta_hat_bins.append(beta_hat_bin)\n",
    "                width_left[i] = curr_SigmaX*np.percentile(\n",
    "                    past_resid, math.ceil(100*beta_hat_bin))\n",
    "                width_right[i] = curr_SigmaX*np.percentile(\n",
    "                    past_resid, math.ceil(100*(1-alpha+beta_hat_bin)))\n",
    "        print(\n",
    "            f'Finish Computing {num_unique_resid} UNIQUE Prediction Intervals, took {time.time()-start} secs.')\n",
    "        # This is because |width|=T1/stride.\n",
    "        width_left = np.repeat(width_left, stride)\n",
    "        # This is because |width|=T1/stride.\n",
    "        width_right = np.repeat(width_right, stride)\n",
    "        PIs_Ensemble = pd.DataFrame(np.c_[out_sample_predict+width_left,\n",
    "                                          out_sample_predict+width_right], columns=['lower', 'upper'])\n",
    "        self.PIs_Ensemble = PIs_Ensemble\n",
    "\n",
    "    '''\n",
    "        All together\n",
    "    '''\n",
    "\n",
    "    def get_results(self, alpha, data_name, itrial, true_Y_predict=[], method='Ensemble'):\n",
    "        '''\n",
    "            NOTE: I added a \"true_Y_predict\" option, which will be used for calibrating coverage under missing data\n",
    "            In particular, this is needed when the Y_predict we use for training is NOT the same as true Y_predict\n",
    "        '''\n",
    "        results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',\n",
    "                                        'method', 'train_size', 'coverage', 'width'])\n",
    "        train_size = len(self.X_train)\n",
    "        if method == 'Ensemble':\n",
    "            PI = self.PIs_Ensemble\n",
    "        Ytest = self.Y_predict.cpu().detach().numpy()\n",
    "        coverage = ((np.array(PI['lower']) <= Ytest) & (\n",
    "            np.array(PI['upper']) >= Ytest)).mean()\n",
    "        if len(true_Y_predict) > 0:\n",
    "            coverage = ((np.array(PI['lower']) <= true_Y_predict) & (\n",
    "                np.array(PI['upper']) >= true_Y_predict)).mean()\n",
    "        print(f'Average Coverage is {coverage}')\n",
    "        width = (PI['upper'] - PI['lower']).mean()\n",
    "        print(f'Average Width is {width}')\n",
    "        results.loc[len(results)] = [itrial, data_name,\n",
    "                                     'torch_MLP', method, train_size, coverage, width]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Aa4w4hK43oBz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CP_LS(X, Y, x, alpha, weights=[], tags=[]):\n",
    "    # Barber et al. 2022: Nex-CP\n",
    "    # weights are used for computing quantiles for the prediction interval\n",
    "    # tags are used as weights in weighted least squares regression\n",
    "    n = len(Y)\n",
    "\n",
    "    if(len(tags) == 0):\n",
    "        tags = np.ones(n+1)\n",
    "\n",
    "    if(len(weights) == 0):\n",
    "        weights = np.ones(n+1)\n",
    "    if(len(weights) == n):\n",
    "        weights = np.r_[weights, 1]\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # randomly permute one weight for the regression\n",
    "    random_ind = int(np.where(np.random.multinomial(1, weights, 1))[1])\n",
    "    tags[np.c_[random_ind, n]] = tags[np.c_[n, random_ind]]\n",
    "\n",
    "    XtX = (X.T*tags[:-1]).dot(X) + np.outer(x, x)*tags[-1]\n",
    "    a = Y - X.dot(np.linalg.solve(XtX, (X.T*tags[:-1]).dot(Y)))\n",
    "    b = -X.dot(np.linalg.solve(XtX, x))*tags[-1]\n",
    "    a1 = -x.T.dot(np.linalg.solve(XtX, (X.T*tags[:-1]).dot(Y)))\n",
    "    b1 = 1 - x.T.dot(np.linalg.solve(XtX, x))*tags[-1]\n",
    "    # if we run weighted least squares on (X[1,],Y[1]),...(X[n,],Y[n]),(x,y)\n",
    "    # then a + b*y = residuals of data points 1,..,n\n",
    "    # and a1 + b1*y = residual of data point n+1\n",
    "\n",
    "    y_knots = np.sort(\n",
    "        np.unique(np.r_[((a-a1)/(b1-b))[b1-b != 0], ((-a-a1)/(b1+b))[b1+b != 0]]))\n",
    "    y_inds_keep = np.where(((np.abs(np.outer(a1+b1*y_knots, np.ones(n)))\n",
    "                             > np.abs(np.outer(np.ones(len(y_knots)), a)+np.outer(y_knots, b))) *\n",
    "                            weights[:-1]).sum(1) <= 1-alpha)[0]\n",
    "    y_PI = np.array([y_knots[y_inds_keep.min()], y_knots[y_inds_keep.max()]])\n",
    "    if(weights[:-1].sum() <= 1-alpha):\n",
    "        y_PI = np.array([-np.inf, np.inf])\n",
    "    return y_PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UPDhYUK_3oBz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Model and data helper ####\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d, sigma=False):\n",
    "        super(MLP, self).__init__()\n",
    "        H = 64\n",
    "        layers = [nn.Linear(d, H), nn.ReLU(), nn.Linear(\n",
    "            H, H), nn.ReLU(), nn.Linear(H, 1)]\n",
    "        self.sigma = sigma\n",
    "        if self.sigma:\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        perturb = 1e-3 if self.sigma else 0\n",
    "        return self.layers(x)+perturb\n",
    "\n",
    "\n",
    "def get_new_data():\n",
    "    ''' Note, the difference from earlier case 3 in paper is that\n",
    "        1) I reduce d from 100 to 20,\n",
    "        2) I let X to be different, so sigmaX differs\n",
    "            The sigmaX is a linear model so this effect in X is immediate\n",
    "        I keep the same AR(1) eps & everything else.'''\n",
    "    def True_mod_nonlinear_pre(feature):\n",
    "        '''\n",
    "        Input:\n",
    "        Output:\n",
    "        Description:\n",
    "            f(feature): R^d -> R\n",
    "        '''\n",
    "        # Attempt 3 Nonlinear model:\n",
    "        # f(X)=sqrt(1+(beta^TX)+(beta^TX)^2+(beta^TX)^3), where 1 is added in case beta^TX is zero\n",
    "        d = len(feature)\n",
    "        np.random.seed(0)\n",
    "        # e.g. 20% of the entries are NON-missing\n",
    "        beta1 = random(1, d, density=0.2).A\n",
    "        betaX = np.abs(beta1.dot(feature))\n",
    "        return (betaX + betaX**2 + betaX**3)**(1/4)\n",
    "    Tot, d = 2000, 20\n",
    "    Fmap = True_mod_nonlinear_pre\n",
    "    # Multiply each random feature by exponential component, which is repeated every Tot/rep elements\n",
    "    rep = 10\n",
    "    mult = np.exp(np.repeat(np.linspace(0, 2, rep), Tot/rep)).reshape(Tot, 1)\n",
    "    X = np.random.rand(Tot, d)*mult\n",
    "    fX = np.array([Fmap(x) for x in X]).flatten()\n",
    "    beta_Sigma = 0.1*np.ones(d)\n",
    "    sigmaX = np.maximum(X.dot(beta_Sigma).T, 0)\n",
    "    with open(f'Data_nochangepts_nonlinear.p', 'rb') as fp:\n",
    "        Data_dc = pickle.load(fp)\n",
    "    eps = Data_dc['Eps']\n",
    "    Y = fX + sigmaX*eps\n",
    "    np.random.seed(1103)\n",
    "    idx = np.random.choice(Tot, Tot, replace=False)\n",
    "    Y, X, fX, sigmaX, eps = Y[idx], X[idx], fX[idx], sigmaX[idx], eps[idx]\n",
    "    return {'Y': torch.from_numpy(Y).float(), 'X': torch.from_numpy(X).float(), 'f(X)': fX, 'sigma(X)': sigmaX, 'Eps': eps}\n",
    "\n",
    "\n",
    "def get_new_data_simple(num_pts, alpha, beta):\n",
    "    '''\n",
    "        Y_t = alpha*Y_{t-1}+\\eps_t\n",
    "        \\eps_t = beta*\\eps_{t-1}+v_t\n",
    "        v_t ~ N(0,1)\n",
    "        So X_t = Y_{t-1}, f(X_t) = alpha*X_t\n",
    "        If t = 0:\n",
    "            X_t = 0, Y_t=\\eps_t = v_t\n",
    "    '''\n",
    "    v0 = torch.randn(1)\n",
    "    Y, X, fX, eps = [v0], [torch.zeros(1)], [torch.zeros(1)], [v0]\n",
    "    scale = torch.sqrt(torch.ones(1)*0.1)\n",
    "    for _ in range(num_pts-1):\n",
    "        vt = torch.randn(1)*scale\n",
    "        X.append(Y[-1])\n",
    "        fX.append(alpha*Y[-1])\n",
    "        eps.append(beta*eps[-1]+vt)\n",
    "        Y.append(fX[-1]+eps[-1])\n",
    "    Y, X, fX, eps = torch.hstack(Y), torch.vstack(\n",
    "        X), torch.vstack(fX), torch.hstack(eps)\n",
    "    return {'Y': Y.float(), 'X': X.float(), 'f(X)': fX, 'Eps': eps}\n",
    "\n",
    "\n",
    "# def electric_dataset():\n",
    "#     # ELEC2 data set\n",
    "#     # downloaded from https://www.kaggle.com/yashsharan/the-elec2-dataset\n",
    "#     data = pd.read_csv('./Data/electricity-normalized.csv')\n",
    "#     print(data.head())\n",
    "#     print(data.isnull().sum()/len(data))\n",
    "#     col_names = data.columns\n",
    "#     print(col_names)\n",
    "#     data = data.to_numpy()\n",
    "\n",
    "#     # remove the first stretch of time where 'transfer' does not vary\n",
    "#     data = data[17760:]\n",
    "\n",
    "#     # set up variables for the task (predicting 'transfer')\n",
    "#     covariate_col = ['nswprice', 'nswdemand', 'vicprice', 'vicdemand']\n",
    "#     response_col = 'transfer'\n",
    "#     # keep data points for 9:00am - 12:00pm\n",
    "#     keep_rows = np.where((data[:, 2] > data[17, 2])\n",
    "#                          & (data[:, 2] < data[24, 2]))[0]\n",
    "\n",
    "#     X = data[keep_rows][:, np.where(\n",
    "#         [t in covariate_col for t in col_names])[0]]\n",
    "#     Y = data[keep_rows][:, np.where(col_names == response_col)[0]].flatten()\n",
    "#     X = X.astype('float64')\n",
    "#     Y = Y.astype('float64')\n",
    "\n",
    "#     return X, Y\n",
    "\n",
    "#### Other helpers ####\n",
    "\n",
    "def rolling_avg(x, window=100):\n",
    "    return np.convolve(x, np.ones(window)/window)[(window-1):-window]\n",
    "\n",
    "\n",
    "def dict_to_latex(dict):\n",
    "    DF = pd.DataFrame.from_dict(np.vstack(dict.values()))\n",
    "    keys = list(dict.keys())\n",
    "    index = np.array([[f'{key} coverage', f'{key} width']\n",
    "                     for key in keys]).flatten()\n",
    "    DF.index = index\n",
    "    DF.columns = train_ls\n",
    "    print(DF)\n",
    "    print(DF.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Cg1OeUk03oB0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sepsisdata = pd.read_csv('./Data/fully_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a6BwJlKQ3oB0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pat_id', 'HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'Glucose',\n",
       "       'Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS',\n",
       "       'SepsisLabel', 'hours2sepsis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsisdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9zWRCFoQ3oB0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sepsisdata.iloc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLWvmf6p3oB1"
   },
   "outputs": [],
   "source": [
    "# Y_full = sepsisdata['hours2sepsis']\n",
    "# X_full = sepsisdata.drop(columns=['pat_id','hours2sepsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVZbEus53oB1"
   },
   "outputs": [],
   "source": [
    "# X_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXrLT5QDvMVH"
   },
   "source": [
    "# Running tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vhYz613WvY4u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "# Whether we save dictionary for plotting rolling cov or width\n",
    "save_dict_rolling = True\n",
    "non_stat_solar = True  # Whether X_t in solar contains time covariate\n",
    "if non_stat_solar == False:\n",
    "    # We only visualize rolling results under non-stationary feature X_t\n",
    "    save_dict_rolling = False\n",
    "# train_ls = [0.5, 0.6, 0.7, 0.8]\n",
    "train_ls = [0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h82M4AKEvcOb"
   },
   "source": [
    "## EnbPI & SPCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jNWExRaNvjAF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Train frac at 0.8\n",
      "Finish Fitting 25 Bootstrap models, took 5.520439386367798 secs.\n",
      "Shape of slided residual lists is (6000, 300)\n",
      "Finish Computing 6000 UNIQUE Prediction Intervals, took 3.9201836585998535 secs.\n",
      "Average Coverage is 0.9183333333333333\n",
      "Average Width is 185.4643907204691\n",
      "########################################\n",
      "Train frac at 0.8\n",
      "Finish Fitting 25 Bootstrap models, took 5.555687665939331 secs.\n",
      "Shape of slided residual lists is (6000, 24000)\n",
      "Width at test 0 is 0.6345496211491586\n",
      "Width at test 300 is 0.6189929124313132\n",
      "Width at test 600 is 0.6227312850128577\n",
      "Width at test 900 is 0.6170601740443686\n",
      "Width at test 1200 is 138.28792974130792\n",
      "Width at test 1500 is 0.6256341523027693\n",
      "Width at test 1800 is 0.6245775639399014\n",
      "Width at test 2100 is 0.6224695766115076\n",
      "Width at test 2400 is 0.6255664073918332\n",
      "Width at test 2700 is 0.6224122524627305\n",
      "Width at test 3000 is 136.92536830880672\n",
      "Width at test 3300 is 0.6162581553489304\n",
      "Width at test 3600 is 0.6128716832117718\n",
      "Width at test 3900 is 0.6081869358551231\n",
      "Width at test 4200 is 0.608132438889502\n",
      "Width at test 4500 is 0.6064438801357888\n",
      "Width at test 4800 is 139.9771728515625\n",
      "Width at test 5100 is 0.6038813249678263\n",
      "Width at test 5400 is 130.80609042900653\n",
      "Width at test 5700 is 0.6070495176225599\n",
      "Finish Computing 6000 UNIQUE Prediction Intervals, took 38870.143953323364 secs.\n",
      "Average Coverage is 0.9751666666666666\n",
      "Average Width is 21.57271291492014\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Arguments:\n",
    "        simulation: bool. True use simulated data. False use solar\n",
    "            simul_type: int. 1 = simple state-space. 2 = non-statioanry. 3 = heteroskedastic\n",
    "            The latter 2 follows from case 3 in paper\n",
    "        conditions: Contain these three below:\n",
    "            use_quantile_regr: bool. True use `quantile_regr`. False use empirical quatile\n",
    "            quantile_regr: str. Which quantile regression to fit residuals (e.g., \"RF\", \"LR\")\n",
    "            fit_func: None or sklearn module with methods `.fit` & `.predict`. If None, use MLP above\n",
    "\n",
    "        fit_sigmaX: bool. True if to fit heteroskedastic errors. ONLY activated if fit_func is NONE (i.e. MLP), because errors are unobserved so `.fit()` does not work\n",
    "\n",
    "        smallT: bool. True if empirical quantile uses not ALL T residual in the past to get quantile (should be tuned as sometimes longer memory causes poor coverage)\n",
    "            past_window: int. If smallT True, EnbPI uses `past_window` most residuals to get width. FOR quantile_regr of residuals, it determines the dimension of the \"feature\" that predict new quantile of residuals autoregressively\n",
    "\n",
    "    Results:\n",
    "        dict: contains dictionary of coverage and width under different training fraction (fix alpha) under various argument combinations\n",
    "\n",
    "'''\n",
    "importlib.reload(sys.modules['utils_quick'])\n",
    "importlib.reload(sys.modules['utils_latest'])\n",
    "dict_full = {}\n",
    "dict_rolling = {}\n",
    "''' Test '''\n",
    "# Conditions: [if simulation, if use quantile regr, which quantile regr]\n",
    "\n",
    "# use quantile residuals: SPCI\n",
    "#use empirical residuals: EnbPI. Naive empirical quantile\n",
    "conditions_real = [[False, ''], [True, 'RF']]\n",
    "# For burn-in period plot\n",
    "PIs_EnbPI = 0\n",
    "PIs_SPCI = 0\n",
    "                   \n",
    "for condition in conditions_real:\n",
    "    use_quantile_regr, quantile_regr = condition\n",
    "    name = 'SPCI' if use_quantile_regr else 'EnbPI'\n",
    "    result_cov, result_width = [], []\n",
    "    for train_frac in train_ls:\n",
    "        print('########################################')\n",
    "        print(f'Train frac at {train_frac}')              \n",
    "        # data_name = 'solar'  # 'electric' or 'solar'\n",
    "        data_name = 'physionet_sepsis'\n",
    "        # if data_name == 'solar':\n",
    "        #         # Get solar data WITH time t as covariate\n",
    "        #     dloader = utils_latest.data_loader()\n",
    "        #     Y_full, X_full_old, X_full_nonstat = dloader.get_non_stationary_real(\n",
    "        #             univariate=False)\n",
    "        #     if non_stat_solar:\n",
    "        #         X_full = X_full_nonstat\n",
    "        #     else:\n",
    "        #         X_full = X_full_old\n",
    "        #     fit_func = RandomForestRegressor(n_estimators=10, criterion='mse',\n",
    "        #                                           bootstrap=False, n_jobs=-1)\n",
    "        #     past_window = 500\n",
    "        # if data_name == 'electric':\n",
    "        #     X_full, Y_full = electric_dataset()\n",
    "        #     fit_func = RandomForestRegressor(n_estimators=10, max_depth=1, criterion='mse',\n",
    "        #                                           bootstrap=False, n_jobs=-1)\n",
    "        #     past_window = 300\n",
    "        if data_name == 'physionet_sepsis':\n",
    "            sepsisdata = pd.read_csv('./Data/fully_imputed.csv')\n",
    "            sepsisdata = sepsisdata[0:30000]\n",
    "            Y_full = sepsisdata['hours2sepsis']\n",
    "            X_full = sepsisdata.drop(columns=['pat_id','hours2sepsis'])\n",
    "            \n",
    "            fit_func = RandomForestRegressor(n_estimators=10, max_depth=1, criterion='mse',\n",
    "                                                  bootstrap=False, n_jobs=-1)\n",
    "            past_window = 300                 \n",
    "                   \n",
    "        Y_full = Y_full.to_numpy()  \n",
    "        X_full = X_full.to_numpy()\n",
    "        Y_full, X_full = torch.from_numpy(Y_full).float().to(\n",
    "                device), torch.from_numpy(X_full).float().to(device)\n",
    "        fit_sigmaX = False\n",
    "        B = 25             \n",
    "                   \n",
    "        alpha, itrial = 0.1, 0\n",
    "        N = int(X_full.shape[0]*train_frac)\n",
    "        X_train, X_predict, Y_train, Y_predict = X_full[:\n",
    "                                                        N], X_full[N:], Y_full[:N], Y_full[N:]\n",
    "\n",
    "        # Train\n",
    "        EnbPI = prediction_interval_with_SPCI(\n",
    "            X_train, X_predict, Y_train, Y_predict, fit_func=fit_func)\n",
    "        EnbPI.fit_bootstrap_models_online(B, fit_sigmaX=fit_sigmaX)\n",
    "        # Under cond quantile, we are ALREADY using the last window for prediction so smallT is True\n",
    "        smallT = not use_quantile_regr\n",
    "        EnbPI.compute_PIs_Ensemble_online(\n",
    "            alpha, smallT=smallT, past_window=past_window, use_quantile_regr=use_quantile_regr,\n",
    "            quantile_regr=quantile_regr)\n",
    "        results = EnbPI.get_results(alpha, data_name, itrial)\n",
    "        result_cov.append(results['coverage'].item())\n",
    "        result_width.append(results['width'].item())\n",
    "        # Lastly, compute rolling width to plot\n",
    "        PI = EnbPI.PIs_Ensemble\n",
    "        if use_quantile_regr:\n",
    "            PIs_SPCI = PI\n",
    "        else:\n",
    "            PIs_EnbPI = PI\n",
    "        Ytest = EnbPI.Y_predict.cpu().detach().numpy()\n",
    "        coverage = ((np.array(PI['lower']) <= Ytest)\n",
    "                    & (np.array(PI['upper']) >= Ytest))\n",
    "        width = ((np.array(PI['upper'])-np.array(PI['lower'])))\n",
    "        # cov_moving = rolling_avg(coverage)\n",
    "        # width_moving = rolling_avg(width)\n",
    "        dict_rolling[f'{name}{np.round(train_frac,2)}'] = np.vstack([\n",
    "            coverage, width])\n",
    "        if save_dict_rolling:\n",
    "            with open(f'EnbPI_rolling_{data_name}.p', 'wb') as fp:\n",
    "                pickle.dump(dict_rolling, fp,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    dict_full[name] = np.vstack([result_cov, result_width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPWsYs6Dz8UT"
   },
   "source": [
    "Due to variability, what you see for EnbPI may be different. However, in general EnbPI intervals are wider\n",
    "\n",
    "0.8 indicates that training fraction as a percentage of total data is 80\\% or 1600 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v6yJdKh8wWxY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       0.8\n",
      "EnbPI coverage    0.918333\n",
      "EnbPI width     185.464391\n",
      "SPCI coverage     0.975167\n",
      "SPCI width       21.572713\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "{} &     0.8 \\\\\n",
      "\\midrule\n",
      "EnbPI coverage &    0.92 \\\\\n",
      "EnbPI width    &  185.46 \\\\\n",
      "SPCI coverage  &    0.98 \\\\\n",
      "SPCI width     &   21.57 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_to_latex(dict_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSawvfzevdfg"
   },
   "source": [
    "## AdaptiveCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jd5tmtpR2Nwm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.24\n",
      "  Using cached scikit_learn-0.24.0-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages (from scikit-learn==0.24) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages (from scikit-learn==0.24) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages (from scikit-learn==0.24) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==0.24)\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "Successfully installed scikit-learn-0.22.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "import PI_class_EnbPI\n",
    "# !pip uninstall skranger\n",
    "# Need for skranger to work properly. Sorry for the possibly avoidable complexity. We assumed one can \n",
    "# get away with this if skranger or scikit-garden is consistently used\n",
    "# May need to restart runtime\n",
    "!pip install scikit-learn==0.24\n",
    "# !pip install scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "G_OLg38n3oB3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# error: cannot build skranger. \n",
    "# !pip install skranger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nv5YRCxW3oB3"
   },
   "outputs": [],
   "source": [
    "from skranger.ensemble import RangerForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_ilWZhc3oB3"
   },
   "outputs": [],
   "source": [
    "# !pip install skranger==\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iQCRvFDvrYY"
   },
   "outputs": [],
   "source": [
    "# NOTE: the variance of this method seems high, and I often need to tune a LOT to avoid yielding very very high coverage.\n",
    "data_name = 'solar'\n",
    "dict_AdaptiveCI = {}\n",
    "dict_rolling_AdaptiveCI = {}\n",
    "cov_ls, width_ls = [], []\n",
    "for train_frac in train_ls:\n",
    "    # As it is split conformal, the result can be random, so we repeat over seed\n",
    "    seeds = [524, 1103, 1111, 1214, 1228]\n",
    "    cov_tmp_ls, width_tmp_ls = [], []\n",
    "    print('########################################')\n",
    "    print(f'Train frac at {train_frac} over {len(seeds)} seeds')\n",
    "    for seed in seeds:\n",
    "        dloader = utils_latest.data_loader()\n",
    "        Y_full, X_full_old, X_full_nonstat = dloader.get_non_stationary_real(\n",
    "            univariate=False)\n",
    "        if non_stat_solar:\n",
    "            X_full = X_full_nonstat\n",
    "        else:\n",
    "            X_full = X_full_old\n",
    "        N = int(X_full.shape[0]*train_frac)\n",
    "        X_train, X_predict, Y_train, Y_predict = X_full[:\n",
    "                                                        N], X_full[N:], Y_full[:N], Y_full[N:]\n",
    "        if non_stat_solar:\n",
    "            # More complex yields wider intervals and more conservative coverage\n",
    "            fit_func = RangerForestRegressor(\n",
    "                n_estimators=5, quantiles=True, seed=seed)\n",
    "        else:\n",
    "            fit_func = RangerForestRegressor(\n",
    "                n_estimators=10, quantiles=True, seed=seed)\n",
    "        PI_test_adaptive = PI_class_EnbPI.QOOB_or_adaptive_CI(\n",
    "            fit_func, X_train, X_predict, Y_train, Y_predict)\n",
    "        AdaptiveCI_result = PI_test_adaptive.compute_AdaptiveCI_intervals(\n",
    "            data_name, 0, l=int(0.75 * X_train.shape[0]),\n",
    "            alpha=alpha)\n",
    "        PIs_AdaptiveCI = PI_test_adaptive.PIs\n",
    "        Ytest = PI_test_adaptive.Y_predict\n",
    "        coverage = ((np.array(PIs_AdaptiveCI['lower']) <= Ytest)\n",
    "                    & (np.array(PIs_AdaptiveCI['upper']) >= Ytest))\n",
    "        width = (\n",
    "            (np.array(PIs_AdaptiveCI['upper'])-np.array(PIs_AdaptiveCI['lower'])))\n",
    "        cov_tmp_ls.append(coverage)\n",
    "        width_tmp_ls.append(width)\n",
    "    coverage = np.vstack(cov_tmp_ls).mean(axis=0)\n",
    "    width = np.vstack(width_tmp_ls).mean(axis=0)\n",
    "    PIs_AdaptiveCI = width  # for plot later\n",
    "    # cov_moving = rolling_avg(coverage)\n",
    "    # width_moving = rolling_avg(width)\n",
    "    dict_rolling_AdaptiveCI[f'placeholder{np.round(train_frac,2)}'] = np.vstack([\n",
    "        coverage, width])\n",
    "    if save_dict_rolling:\n",
    "        with open(f'AdaptiveCI_rolling_{data_name}.p', 'wb') as fp:\n",
    "            pickle.dump(dict_rolling_AdaptiveCI, fp,\n",
    "                        protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    cov_ls.append(np.mean(coverage))\n",
    "    width_ls.append(np.mean(width))\n",
    "dict_AdaptiveCI['AdaptiveCI'] = np.vstack([cov_ls, width_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PDVv1Gr2x48"
   },
   "outputs": [],
   "source": [
    "dict_to_latex(dict_AdaptiveCI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5WGZbUkvdmG"
   },
   "source": [
    "## Nex-CP WLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q7TzGQlvyo8"
   },
   "outputs": [],
   "source": [
    "cov, width = [], []\n",
    "dict_full = {}\n",
    "dict_rolling = {}\n",
    "data_name = 'solar'  # Eletric or solar\n",
    "if data_name == 'solar':\n",
    "    # Get solar data WITH time t as covariate\n",
    "    dloader = utils_latest.data_loader()\n",
    "    Y_full, X_full_old, X_full_nonstat = dloader.get_non_stationary_real(\n",
    "        univariate=False)\n",
    "    if non_stat_solar:\n",
    "        X_full = X_full_nonstat\n",
    "    else:\n",
    "        X_full = X_full_old\n",
    "if data_name == 'electric':\n",
    "    X_full, Y_full = electric_dataset()\n",
    "N = len(X_full)\n",
    "for train_frac in train_ls:\n",
    "    train_size = int(train_frac*N)\n",
    "    PI_nexCP_WLS = np.zeros((N, 2))\n",
    "    for n in np.arange(train_size, N):\n",
    "        # weights and tags (parameters for new methods)\n",
    "        rho = 0.99\n",
    "        rho_LS = 0.99\n",
    "        weights = rho**(np.arange(n, 0, -1))\n",
    "        tags = rho_LS**(np.arange(n, -1, -1))\n",
    "        PI_nexCP_WLS[n, :] = CP_LS(X_full[:n, :], Y_full[:n], X_full[n, :], alpha,\n",
    "                                    weights=weights, tags=tags)\n",
    "        inc = int((N-train_size)/20)\n",
    "        if (n-train_size) % inc == 0:\n",
    "            print(\n",
    "                f'NEX-CP WLS width at {n-train_size} is: {PI_nexCP_WLS[n,1] - PI_nexCP_WLS[n,0]}')\n",
    "    cov_nexCP_WLS = (PI_nexCP_WLS[train_size:, 0] <= Y_full[train_size:N]) *\\\n",
    "        (PI_nexCP_WLS[train_size:, 1] >= Y_full[train_size:N])\n",
    "    PI_width_nexCP_WLS = PI_nexCP_WLS[train_size:,\n",
    "                                      1]-PI_nexCP_WLS[train_size:, 0]\n",
    "    cov.append(np.mean(cov_nexCP_WLS))\n",
    "    width.append(np.mean(PI_width_nexCP_WLS))\n",
    "    print(\n",
    "        f'At {train_frac} tot data \\n cov: {cov[-1]} & width: {width[-1]}')\n",
    "    # Rolling coverage and width\n",
    "    # cov_moving = rolling_avg(cov_nexCP_WLS)\n",
    "    # width_moving = rolling_avg(PI_width_nexCP_WLS)\n",
    "    dict_rolling[f'placeholder{train_frac}'] = np.vstack(\n",
    "        [cov_nexCP_WLS, PI_width_nexCP_WLS])\n",
    "    if save_dict_rolling:\n",
    "        with open(f'NEXCP_rolling_{data_name}.p', 'wb') as fp:\n",
    "            pickle.dump(dict_rolling, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "dict_full = {'Nex-CP WLS': np.vstack([cov, width])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdJogtc3271d"
   },
   "outputs": [],
   "source": [
    "dict_to_latex(dict_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U6PIRbGvhZI"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sdrgB6Dv6bP"
   },
   "source": [
    "## Burn-in plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_73dOA8w8tyn"
   },
   "outputs": [],
   "source": [
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o9tVyPb6Uuw"
   },
   "outputs": [],
   "source": [
    "with open(f'EnbPI_rolling_{data_name}.p', 'rb') as fp:\n",
    "    dict_rolling = pickle.load(fp)\n",
    "with open(f'NEXCP_rolling_{data_name}.p', 'rb') as fp:\n",
    "    NEXCP_dict_rolling = pickle.load(fp)\n",
    "with open(f'AdaptiveCI_rolling_{data_name}.p', 'rb') as fp:\n",
    "    AdaptiveCI_dict_rolling = pickle.load(fp)\n",
    "vanilla_cov_rolling, vanilla_width_rolling = dict_rolling['EnbPI0.8']\n",
    "predictive_cov_rolling, predictive_width_rolling = dict_rolling['SPCI0.8']\n",
    "NEXCP_cov_rolling, NEXCP_width_rolling = NEXCP_dict_rolling['placeholder0.8']\n",
    "AdaptiveCI_cov_rolling, AdaptiveCI_width_rolling = AdaptiveCI_dict_rolling[\n",
    "    'placeholder0.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTB7X3QdyZGi"
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "first = vanilla_width_rolling[:window_size]\n",
    "ax.plot(first, label=f'EnbPI: {np.round(first.mean(),2)}', color='black')\n",
    "second = predictive_width_rolling[:window_size]\n",
    "ax.plot(second, label=f'SPCI: {np.round(second.mean(),2)}', color='orange')\n",
    "third = AdaptiveCI_width_rolling[:window_size]\n",
    "ax.plot(\n",
    "    third, label=f'AdaptiveCI: {np.round(third.mean(),2)}', color='gray', linewidth=0.75)\n",
    "fourth = NEXCP_width_rolling[:window_size]\n",
    "ax.plot(\n",
    "    fourth, label=f'Nex-CP WLS: {np.round(fourth.mean(),2)}', color='magenta')\n",
    "ax.set_xlabel('Burn-in Period')\n",
    "ax.set_ylabel('Width')\n",
    "ax.legend(title='Method: Ave Width in burn-in', title_fontsize=17,\n",
    "          loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.525))\n",
    "suffix = 'non_stat_' if non_stat_solar else ''\n",
    "plt.savefig(f'Brun_in_plot_{suffix}{data_name}.pdf', dpi=300,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swTYh0rwv-Me"
   },
   "source": [
    "## Rolling cov and width after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86Gvw_IFyh_W"
   },
   "outputs": [],
   "source": [
    "dsets = ['solar']\n",
    "methods = ['NEXCP+AdaptiveCI']\n",
    "for data_name in dsets:\n",
    "    for compete in methods:\n",
    "        if data_name == 'electric' and compete == 'AdaptiveCI':\n",
    "            continue\n",
    "        print(f'EnbPI vs. {compete} on {data_name}')\n",
    "        if data_name == 'solar':\n",
    "            # Get solar data WITH time t as covariate\n",
    "            dloader = utils_latest.data_loader()\n",
    "            Y_full, X_full_old, X_full_nonstat = dloader.get_non_stationary_real(\n",
    "                univariate=False)\n",
    "            if non_stat_solar:\n",
    "                X_full = X_full_nonstat\n",
    "            else:\n",
    "                X_full = X_full_old\n",
    "        vanilla_cov_rolling, vanilla_width_rolling = rolling_avg(\n",
    "            vanilla_cov_rolling, window_size), rolling_avg(vanilla_width_rolling, window_size)\n",
    "        predictive_cov_rolling, predictive_width_rolling = rolling_avg(\n",
    "            predictive_cov_rolling, window_size), rolling_avg(predictive_width_rolling, window_size)\n",
    "        NEXCP_cov_rolling, NEXCP_width_rolling = rolling_avg(\n",
    "            NEXCP_cov_rolling, window_size), rolling_avg(NEXCP_width_rolling, window_size)\n",
    "        AdaptiveCI_cov_rolling, AdaptiveCI_width_rolling = rolling_avg(\n",
    "            AdaptiveCI_cov_rolling, window_size), rolling_avg(AdaptiveCI_width_rolling, window_size)\n",
    "        N = len(Y_full)\n",
    "        N0 = int(0.8*len(X_full))\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 4), sharex=True)\n",
    "        ax[0].axhline(y=1-alpha, linestyle='--', color='gray')\n",
    "        xaxis = np.arange(N0+window_size, N)\n",
    "        ax[0].plot(xaxis, vanilla_cov_rolling,\n",
    "                    color='black', label='EnbPI')\n",
    "        color_tmp = 'orange'\n",
    "        ax[0].plot(xaxis, predictive_cov_rolling,\n",
    "                    color=color_tmp, label='SPCI')\n",
    "        ax[0].plot(xaxis, AdaptiveCI_cov_rolling,\n",
    "                    color='gray', label='AdaptiveCI')\n",
    "        ax[0].plot(xaxis, NEXCP_cov_rolling,\n",
    "                    color='magenta', label='Nex-CP WLS')\n",
    "        ax[0].set_xlabel('Data index')\n",
    "        ax[1].plot(xaxis, AdaptiveCI_width_rolling, color='gray')\n",
    "        ax[1].plot(xaxis, NEXCP_width_rolling, color='magenta')\n",
    "        ax[0].set_ylim([1-4*alpha, 1])\n",
    "        ax[0].set_ylabel('Rolling coverage')\n",
    "        ax[0].legend(ncol=2, loc='lower center')\n",
    "        ax[1].plot(xaxis, vanilla_width_rolling, color='black')\n",
    "        ax[1].plot(xaxis, predictive_width_rolling, color=color_tmp)\n",
    "        ax[1].set_ylabel('Rolling width')\n",
    "        ax[1].set_xlabel('Data index')\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f'Rolling_comparison_{compete}_{data_name}.pdf', dpi=300,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
