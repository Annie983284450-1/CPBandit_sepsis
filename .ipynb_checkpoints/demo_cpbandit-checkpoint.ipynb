{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KZWdnDfi3oBu",
    "outputId": "58f81cd7-c4f8-4df1-e2d3-e9b17825f1c5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "c:\\Users\\98328\\anaconda3\\envs\\py37sepsis\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import skgarden\n",
    "import sys\n",
    "# path = '/content/drive/MyDrive/EnbPI' # You are free to replace this with where you place this file on drive\n",
    "# sys.path.insert(0,path)\n",
    "\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import importlib as ipb\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "import math\n",
    "import time as time\n",
    "import utils_quick as util\n",
    "import utils_latest\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from skgarden import RandomForestQuantileRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/fully_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.045029</td>\n",
       "      <td>97.403481</td>\n",
       "      <td>36.708390</td>\n",
       "      <td>124.795742</td>\n",
       "      <td>85.718264</td>\n",
       "      <td>67.642071</td>\n",
       "      <td>18.833348</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p119046</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.644472</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p119046</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>36.662368</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p119046</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>36.651706</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_id         HR      O2Sat       Temp         SBP         MAP  \\\n",
       "0  p119046  84.045029  97.403481  36.708390  124.795742   85.718264   \n",
       "1  p119046  85.000000  98.000000  36.644472  168.000000  123.000000   \n",
       "2  p119046  81.000000  97.500000  36.662368  163.500000  119.000000   \n",
       "3  p119046  83.000000  98.000000  36.500000  158.000000  117.000000   \n",
       "4  p119046  84.000000  97.000000  36.651706  154.000000  113.000000   \n",
       "\n",
       "         DBP       Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0  67.642071  18.833348  131.356454  60.0       1    1.0    0.0        -7.79   \n",
       "1  94.000000  22.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "2  90.000000  24.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "3  89.000000  22.500000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "4  85.000000  20.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "\n",
       "   ICULOS  SepsisLabel  hours2sepsis  \n",
       "0     1.0            0           500  \n",
       "1     2.0            0           500  \n",
       "2     3.0            0           500  \n",
       "3     4.0            0           500  \n",
       "4     5.0            0           500  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjuucWAivJOC"
   },
   "source": [
    "# Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2blvGaelQXAO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Main Class ####\n",
    "class prediction_interval_with_SPCI():\n",
    "    '''\n",
    "        Create prediction intervals assuming Y_t = f(X_t) + \\sigma(X_t)\\eps_t\n",
    "        Currently, assume the regression function is by default MLP implemented with PyTorch, as it needs to estimate BOTH f(X_t) and \\sigma(X_t), \n",
    "        where the latter is impossible to estimate using scikit-learn modules\n",
    "        Most things carry out, except that we need to have different estimators for f and \\sigma.\n",
    "        fit_func = None: use MLP above\n",
    "    '''\n",
    "     \n",
    "    def __init__(self, X_train, X_predict, Y_train, Y_predict, fit_func=None):\n",
    "        self.regressor = fit_func\n",
    "        self.X_train = X_train\n",
    "        self.X_predict = X_predict\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_predict = Y_predict\n",
    "        # Predicted training data centers by EnbPI\n",
    "        self.Ensemble_train_interval_centers = []\n",
    "        self.Ensemble_train_interval_sigma = []\n",
    "        # Predicted test data centers by EnbPI\n",
    "        self.Ensemble_pred_interval_centers = []\n",
    "        self.Ensemble_pred_interval_sigma = []\n",
    "        self.Ensemble_online_resid = []  # LOO scores\n",
    "        self.beta_hat_bins = []\n",
    "\n",
    "    def fit_bootstrap_models_online(self, B, miss_test_idx=[], fit_sigmaX=True):\n",
    "        '''\n",
    "          Train B bootstrap estimators from subsets of (X_train, Y_train), compute aggregated predictors, and compute the residuals\n",
    "          fit_sigmaX: If False, just avoid predicting \\sigma(X_t) by defaulting it to 1\n",
    "        '''\n",
    "        n, d = self.X_train.shape\n",
    "        n1 = len(self.X_predict)\n",
    "        # hold indices of training data for each f^b\n",
    "        #\n",
    "        boot_samples_idx = util.generate_bootstrap_samples(n, n, B)\n",
    "        # hold predictions from each f^b for fX and sigma&b for sigma\n",
    "        boot_predictionsFX = torch.zeros(B, n+n1).to(device)\n",
    "        boot_predictionsSigmaX = torch.ones(B, n+n1).to(device)\n",
    "        # for i^th column, it shows which f^b uses i in training (so exclude in aggregation)\n",
    "        in_boot_sample = np.zeros((B, n), dtype=bool)\n",
    "        out_sample_predictFX = torch.zeros(n, n1).to(device)\n",
    "        out_sample_predictSigmaX = torch.ones(n, n1).to(device)\n",
    "        start = time.time()\n",
    "        Xfull = torch.vstack([self.X_train, self.X_predict])\n",
    "        for b in range(B):\n",
    "            Xboot, Yboot = self.X_train[boot_samples_idx[b],\n",
    "                                        :], self.Y_train[boot_samples_idx[b], ]\n",
    "            in_boot_sample[b, boot_samples_idx[b]] = True\n",
    "            if self.regressor.__class__.__name__ == 'NoneType':\n",
    "                start1 = time.time()\n",
    "                model_f = MLP(d).to(device)\n",
    "                optimizer_f = torch.optim.Adam(model_f.parameters(), lr=1e-3)\n",
    "                if fit_sigmaX:\n",
    "                    model_sigma = MLP(d, sigma=True).to(device)\n",
    "                    optimizer_sigma = torch.optim.Adam(\n",
    "                        model_sigma.parameters(), lr=2e-3)\n",
    "                for epoch in range(300):\n",
    "                    fXhat = model_f(Xboot)\n",
    "                    sigmaXhat = torch.ones(len(fXhat)).to(device)\n",
    "                    if fit_sigmaX:\n",
    "                        sigmaXhat = model_sigma(Xboot)\n",
    "                    loss = ((Yboot-fXhat)/sigmaXhat).pow(2).mean()/2\n",
    "                    optimizer_f.zero_grad()\n",
    "                    if fit_sigmaX:\n",
    "                        optimizer_sigma.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_f.step()\n",
    "                    if fit_sigmaX:\n",
    "                        optimizer_sigma.step()\n",
    "                with torch.no_grad():\n",
    "                    boot_predictionsFX[b] = model_f(Xfull).flatten()\n",
    "                    if fit_sigmaX:\n",
    "                        boot_predictionsSigmaX[b] = model_sigma(\n",
    "                            Xfull).flatten()\n",
    "                print(\n",
    "                    f'Took {time.time()-start1} secs to finish the {b}th boostrap model')\n",
    "            else:\n",
    "                model = self.regressor\n",
    "                model.fit(Xboot, Yboot)\n",
    "                boot_predictionsFX[b] = torch.from_numpy(\n",
    "                    model.predict(Xfull).flatten()).to(device)\n",
    "                # NOTE, NO sigma estimation because these methods by deFAULT are fitting Y, but we have no observation of errors\n",
    "        print(\n",
    "            f'Finish Fitting {B} Bootstrap models, took {time.time()-start} secs.')\n",
    "        start = time.time()\n",
    "        keep = []\n",
    "        for i in range(n):\n",
    "            b_keep = np.argwhere(~(in_boot_sample[:, i])).reshape(-1)\n",
    "            pred_iFX = boot_predictionsFX[b_keep, i].mean()\n",
    "            pred_iSigmaX = boot_predictionsSigmaX[b_keep, i].mean()\n",
    "            pred_testFX = boot_predictionsFX[b_keep, n:].mean(0)\n",
    "            pred_testSigmaX = boot_predictionsSigmaX[b_keep, n:].mean(0)\n",
    "            if(len(b_keep) > 0):\n",
    "                self.Ensemble_train_interval_centers.append(pred_iFX)\n",
    "                self.Ensemble_train_interval_sigma.append(pred_iSigmaX)\n",
    "                resid_LOO = (self.Y_train[i] - pred_iFX)/pred_iSigmaX\n",
    "                out_sample_predictFX[i] = pred_testFX\n",
    "                out_sample_predictSigmaX[i] = pred_testSigmaX\n",
    "                keep = keep+[b_keep]\n",
    "            self.Ensemble_online_resid.append(resid_LOO.item())\n",
    "        sorted_out_sample_predictFX = out_sample_predictFX.mean(0)  # length n1\n",
    "        sorted_out_sample_predictSigmaX = out_sample_predictSigmaX.mean(\n",
    "            0)  # length n1\n",
    "        resid_out_sample = (\n",
    "            self.Y_predict-sorted_out_sample_predictFX)/sorted_out_sample_predictSigmaX\n",
    "        if len(miss_test_idx) > 0:\n",
    "            # Replace missing residuals with that from the immediate predecessor that is not missing, as\n",
    "            # o/w we are not assuming prediction data are missing\n",
    "            for idx in range(len(miss_test_idx)):\n",
    "                i = miss_test_idx[idx]\n",
    "                if i > 0:\n",
    "                    j = i-1\n",
    "                    while j in miss_test_idx[:idx]:\n",
    "                        j -= 1\n",
    "                    resid_out_sample[i] = resid_out_sample[j]\n",
    "\n",
    "                else:\n",
    "                    # The first Y during testing is missing, let it be the last of the training residuals\n",
    "                    # note, training data already takes out missing values, so doing is is fine\n",
    "                    resid_out_sample[0] = self.Ensemble_online_resid[-1]\n",
    "        self.Ensemble_online_resid = np.append(\n",
    "            self.Ensemble_online_resid, resid_out_sample.cpu().detach().numpy())\n",
    "        # print(f'Finish Computing LOO residuals, took {time.time()-start} secs.')\n",
    "        # print(f'Max LOO test residual is {np.max(self.Ensemble_online_resid[n:])}')\n",
    "        # print(f'Min LOO test residual is {np.min(self.Ensemble_online_resid[n:])}')\n",
    "        self.Ensemble_pred_interval_centers = sorted_out_sample_predictFX\n",
    "        self.Ensemble_pred_interval_sigma = sorted_out_sample_predictSigmaX\n",
    "\n",
    "    def compute_PIs_Ensemble_online(self, alpha, stride=1, smallT=True, past_window=100, use_quantile_regr=False, quantile_regr='RF'):\n",
    "        '''\n",
    "            smallT: if True, we would only start with the last n number of LOO residuals, rather than use the full length T ones. Used in change detection\n",
    "                NOTE: smallT can be important if time-series is very dynamic, in which case training MORE data may actaully be worse (because quantile longer)\n",
    "                HOWEVER, if fit quantile regression, set it to be FALSE because we want to have many training pts for the quantile regressor\n",
    "            use_quantile_regr: if True, we fit conditional quantile to compute the widths, rather than simply using empirical quantile\n",
    "        '''\n",
    "        n1 = len(self.X_train)\n",
    "        if smallT:\n",
    "            n1 = min(past_window, len(self.X_train))\n",
    "        # Now f^b and LOO residuals have been constructed from earlier\n",
    "        out_sample_predict = self.Ensemble_pred_interval_centers.cpu().detach().numpy()\n",
    "        out_sample_predictSigmaX = self.Ensemble_pred_interval_sigma\n",
    "        start = time.time()\n",
    "        # Matrix, where each row is a UNIQUE slice of residuals with length stride.\n",
    "        resid_strided = util.strided_app(\n",
    "            self.Ensemble_online_resid[len(self.X_train)-n1:-1], n1, stride)\n",
    "        print(f'Shape of slided residual lists is {resid_strided.shape}')\n",
    "        num_unique_resid = resid_strided.shape[0]\n",
    "        width_left = np.zeros(num_unique_resid)\n",
    "        width_right = np.zeros(num_unique_resid)\n",
    "        # # NEW, alpha becomes alpha_t. Uncomment things below if we decide to use this upgraded EnbPI\n",
    "        # alpha_t = alpha\n",
    "        # errs = []\n",
    "        # gamma = 0.005\n",
    "        # method = 'simple'  # 'simple' or 'complex'\n",
    "        # self.alphas = []\n",
    "        # NOTE: 'max_features='log2', max_depth=2' make the model \"simpler\", which improves performance in practice\n",
    "        for i in range(num_unique_resid):\n",
    "            # for p in range(stride):  # NEW for adaptive alpha\n",
    "            past_resid = resid_strided[i, :]\n",
    "            curr_SigmaX = out_sample_predictSigmaX[i].item()\n",
    "            if use_quantile_regr:\n",
    "                # New predicted conditional quntile\n",
    "                # 1. Get \"past_resid\" into an auto-regressive fashion\n",
    "                # This should be more carefully examined, b/c it depends on how long \\hat{\\eps}_t depends on the past\n",
    "                # From practice, making it small make intervals wider\n",
    "                n2 = past_window\n",
    "                residX = sliding_window_view(past_resid, window_shape=n2)\n",
    "                residY = past_resid[n2:]\n",
    "                # 2. Fit the model. Default quantile regressor is the quantile RF from\n",
    "                # scikit-garden: https://scikit-garden.github.io/\n",
    "                # NOTE, should NOT warm start, as it makes result poor, although training is longer\n",
    "                if quantile_regr == 'RF':\n",
    "                    rfqr = RandomForestQuantileRegressor(\n",
    "                        max_depth=2, random_state=0)\n",
    "                    rfqr.fit(residX[:-1], residY)\n",
    "                    # 3. Find best \\hat{\\beta} via evaluating many quantiles\n",
    "                    beta_hat_bin = util.binning_use_RF_quantile_regr(\n",
    "                        rfqr, residX[-1], alpha)\n",
    "                    width_left[i] = curr_SigmaX*rfqr.predict(\n",
    "                        residX[-1].reshape(1, -1), math.ceil(100 * beta_hat_bin))\n",
    "                    width_right[i] = curr_SigmaX*rfqr.predict(\n",
    "                        residX[-1].reshape(1, -1), math.ceil(100 * (1-alpha+beta_hat_bin)))\n",
    "                # if quantile_regr == 'LR':\n",
    "                #     start1 = time.time()\n",
    "                #     wleft, wright = util.binning_use_linear_quantile_regr(\n",
    "                #         residX, residY, alpha)\n",
    "                #     if i == 0:\n",
    "                #         print(\n",
    "                #             f'100 Linear QRegr approx. takes {100*(time.time()-start1)} secs.')\n",
    "                #     width_left[i] = curr_SigmaX*wleft\n",
    "                #     width_right[i] = curr_SigmaX*wright\n",
    "                if i % int(num_unique_resid/20) == 0:\n",
    "                    print(\n",
    "                        f'Width at test {i} is {width_right[i]-width_left[i]}')\n",
    "            else:\n",
    "                # Naive empirical quantile\n",
    "                # The number of bins will be determined INSIDE binning\n",
    "                beta_hat_bin = util.binning(past_resid, alpha)\n",
    "                # beta_hat_bin = util.binning(past_resid, alpha_t)\n",
    "                self.beta_hat_bins.append(beta_hat_bin)\n",
    "                width_left[i] = curr_SigmaX*np.percentile(\n",
    "                    past_resid, math.ceil(100*beta_hat_bin))\n",
    "                width_right[i] = curr_SigmaX*np.percentile(\n",
    "                    past_resid, math.ceil(100*(1-alpha+beta_hat_bin)))\n",
    "        print(\n",
    "            f'Finish Computing {num_unique_resid} UNIQUE Prediction Intervals, took {time.time()-start} secs.')\n",
    "        # This is because |width|=T1/stride.\n",
    "        width_left = np.repeat(width_left, stride)\n",
    "        # This is because |width|=T1/stride.\n",
    "        width_right = np.repeat(width_right, stride)\n",
    "        PIs_Ensemble = pd.DataFrame(np.c_[out_sample_predict+width_left,\n",
    "                                          out_sample_predict+width_right], columns=['lower', 'upper'])\n",
    "        self.PIs_Ensemble = PIs_Ensemble\n",
    "\n",
    "    '''\n",
    "        All together\n",
    "    '''\n",
    "\n",
    "    def get_results(self, alpha, data_name, itrial, true_Y_predict=[], method='Ensemble'):\n",
    "        '''\n",
    "            NOTE: I added a \"true_Y_predict\" option, which will be used for calibrating coverage under missing data\n",
    "            In particular, this is needed when the Y_predict we use for training is NOT the same as true Y_predict\n",
    "        '''\n",
    "        results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',\n",
    "                                        'method', 'train_size', 'coverage', 'width'])\n",
    "        train_size = len(self.X_train)\n",
    "        if method == 'Ensemble':\n",
    "            PI = self.PIs_Ensemble\n",
    "        Ytest = self.Y_predict.cpu().detach().numpy()\n",
    "        coverage = ((np.array(PI['lower']) <= Ytest) & (\n",
    "            np.array(PI['upper']) >= Ytest)).mean()\n",
    "        if len(true_Y_predict) > 0:\n",
    "            coverage = ((np.array(PI['lower']) <= true_Y_predict) & (\n",
    "                np.array(PI['upper']) >= true_Y_predict)).mean()\n",
    "        print(f'Average Coverage is {coverage}')\n",
    "        width = (PI['upper'] - PI['lower']).mean()\n",
    "        print(f'Average Width is {width}')\n",
    "        results.loc[len(results)] = [itrial, data_name,\n",
    "                                     'torch_MLP', method, train_size, coverage, width]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Aa4w4hK43oBz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CP_LS(X, Y, x, alpha, weights=[], tags=[]):\n",
    "    # Barber et al. 2022: Nex-CP\n",
    "    # weights are used for computing quantiles for the prediction interval\n",
    "    # tags are used as weights in weighted least squares regression\n",
    "    n = len(Y)\n",
    "\n",
    "    if(len(tags) == 0):\n",
    "        tags = np.ones(n+1)\n",
    "\n",
    "    if(len(weights) == 0):\n",
    "        weights = np.ones(n+1)\n",
    "    if(len(weights) == n):\n",
    "        weights = np.r_[weights, 1]\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # randomly permute one weight for the regression\n",
    "    random_ind = int(np.where(np.random.multinomial(1, weights, 1))[1])\n",
    "    tags[np.c_[random_ind, n]] = tags[np.c_[n, random_ind]]\n",
    "\n",
    "    XtX = (X.T*tags[:-1]).dot(X) + np.outer(x, x)*tags[-1]\n",
    "    a = Y - X.dot(np.linalg.solve(XtX, (X.T*tags[:-1]).dot(Y)))\n",
    "    b = -X.dot(np.linalg.solve(XtX, x))*tags[-1]\n",
    "    a1 = -x.T.dot(np.linalg.solve(XtX, (X.T*tags[:-1]).dot(Y)))\n",
    "    b1 = 1 - x.T.dot(np.linalg.solve(XtX, x))*tags[-1]\n",
    "    # if we run weighted least squares on (X[1,],Y[1]),...(X[n,],Y[n]),(x,y)\n",
    "    # then a + b*y = residuals of data points 1,..,n\n",
    "    # and a1 + b1*y = residual of data point n+1\n",
    "\n",
    "    y_knots = np.sort(\n",
    "        np.unique(np.r_[((a-a1)/(b1-b))[b1-b != 0], ((-a-a1)/(b1+b))[b1+b != 0]]))\n",
    "    y_inds_keep = np.where(((np.abs(np.outer(a1+b1*y_knots, np.ones(n)))\n",
    "                             > np.abs(np.outer(np.ones(len(y_knots)), a)+np.outer(y_knots, b))) *\n",
    "                            weights[:-1]).sum(1) <= 1-alpha)[0]\n",
    "    y_PI = np.array([y_knots[y_inds_keep.min()], y_knots[y_inds_keep.max()]])\n",
    "    if(weights[:-1].sum() <= 1-alpha):\n",
    "        y_PI = np.array([-np.inf, np.inf])\n",
    "    return y_PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UPDhYUK_3oBz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Model and data helper ####\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d, sigma=False):\n",
    "        super(MLP, self).__init__()\n",
    "        H = 64\n",
    "        layers = [nn.Linear(d, H), nn.ReLU(), nn.Linear(\n",
    "            H, H), nn.ReLU(), nn.Linear(H, 1)]\n",
    "        self.sigma = sigma\n",
    "        if self.sigma:\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        perturb = 1e-3 if self.sigma else 0\n",
    "        return self.layers(x)+perturb\n",
    "\n",
    "\n",
    "def get_new_data():\n",
    "    ''' Note, the difference from earlier case 3 in paper is that\n",
    "        1) I reduce d from 100 to 20,\n",
    "        2) I let X to be different, so sigmaX differs\n",
    "            The sigmaX is a linear model so this effect in X is immediate\n",
    "        I keep the same AR(1) eps & everything else.'''\n",
    "    def True_mod_nonlinear_pre(feature):\n",
    "        '''\n",
    "        Input:\n",
    "        Output:\n",
    "        Description:\n",
    "            f(feature): R^d -> R\n",
    "        '''\n",
    "        # Attempt 3 Nonlinear model:\n",
    "        # f(X)=sqrt(1+(beta^TX)+(beta^TX)^2+(beta^TX)^3), where 1 is added in case beta^TX is zero\n",
    "        d = len(feature)\n",
    "        np.random.seed(0)\n",
    "        # e.g. 20% of the entries are NON-missing\n",
    "        beta1 = random(1, d, density=0.2).A\n",
    "        betaX = np.abs(beta1.dot(feature))\n",
    "        return (betaX + betaX**2 + betaX**3)**(1/4)\n",
    "    Tot, d = 2000, 20\n",
    "    Fmap = True_mod_nonlinear_pre\n",
    "    # Multiply each random feature by exponential component, which is repeated every Tot/rep elements\n",
    "    rep = 10\n",
    "    mult = np.exp(np.repeat(np.linspace(0, 2, rep), Tot/rep)).reshape(Tot, 1)\n",
    "    X = np.random.rand(Tot, d)*mult\n",
    "    fX = np.array([Fmap(x) for x in X]).flatten()\n",
    "    beta_Sigma = 0.1*np.ones(d)\n",
    "    sigmaX = np.maximum(X.dot(beta_Sigma).T, 0)\n",
    "    with open(f'Data_nochangepts_nonlinear.p', 'rb') as fp:\n",
    "        Data_dc = pickle.load(fp)\n",
    "    eps = Data_dc['Eps']\n",
    "    Y = fX + sigmaX*eps\n",
    "    np.random.seed(1103)\n",
    "    idx = np.random.choice(Tot, Tot, replace=False)\n",
    "    Y, X, fX, sigmaX, eps = Y[idx], X[idx], fX[idx], sigmaX[idx], eps[idx]\n",
    "    return {'Y': torch.from_numpy(Y).float(), 'X': torch.from_numpy(X).float(), 'f(X)': fX, 'sigma(X)': sigmaX, 'Eps': eps}\n",
    "\n",
    "\n",
    "def get_new_data_simple(num_pts, alpha, beta):\n",
    "    '''\n",
    "        Y_t = alpha*Y_{t-1}+\\eps_t\n",
    "        \\eps_t = beta*\\eps_{t-1}+v_t\n",
    "        v_t ~ N(0,1)\n",
    "        So X_t = Y_{t-1}, f(X_t) = alpha*X_t\n",
    "        If t = 0:\n",
    "            X_t = 0, Y_t=\\eps_t = v_t\n",
    "    '''\n",
    "    v0 = torch.randn(1)\n",
    "    Y, X, fX, eps = [v0], [torch.zeros(1)], [torch.zeros(1)], [v0]\n",
    "    scale = torch.sqrt(torch.ones(1)*0.1)\n",
    "    for _ in range(num_pts-1):\n",
    "        vt = torch.randn(1)*scale\n",
    "        X.append(Y[-1])\n",
    "        fX.append(alpha*Y[-1])\n",
    "        eps.append(beta*eps[-1]+vt)\n",
    "        Y.append(fX[-1]+eps[-1])\n",
    "    Y, X, fX, eps = torch.hstack(Y), torch.vstack(\n",
    "        X), torch.vstack(fX), torch.hstack(eps)\n",
    "    return {'Y': Y.float(), 'X': X.float(), 'f(X)': fX, 'Eps': eps}\n",
    "\n",
    "\n",
    "\n",
    "#### Other helpers ####\n",
    "\n",
    "def rolling_avg(x, window=100):\n",
    "    return np.convolve(x, np.ones(window)/window)[(window-1):-window]\n",
    "\n",
    "\n",
    "def dict_to_latex(dict):\n",
    "    DF = pd.DataFrame.from_dict(np.vstack(dict.values()))\n",
    "    keys = list(dict.keys())\n",
    "    index = np.array([[f'{key} coverage', f'{key} width']\n",
    "                     for key in keys]).flatten()\n",
    "    DF.index = index\n",
    "    DF.columns = train_ls\n",
    "    print(DF)\n",
    "    print(DF.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXrLT5QDvMVH"
   },
   "source": [
    "# Running tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p106715', 'p014397', 'p017882', ..., 'p008554', 'p112763',\n",
       "       'p006260'], dtype='<U7')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = np.load('./Data/test_set.npy')\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sepsis = np.load('./Data/train_sepsis.npy')\n",
    "train_nosepsis = np.load('./Data/train_nosepsis.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.045029</td>\n",
       "      <td>97.403481</td>\n",
       "      <td>36.708390</td>\n",
       "      <td>124.795742</td>\n",
       "      <td>85.718264</td>\n",
       "      <td>67.642071</td>\n",
       "      <td>18.833348</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p119046</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.644472</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p119046</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>36.662368</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p119046</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>36.651706</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_id         HR      O2Sat       Temp         SBP         MAP  \\\n",
       "0  p119046  84.045029  97.403481  36.708390  124.795742   85.718264   \n",
       "1  p119046  85.000000  98.000000  36.644472  168.000000  123.000000   \n",
       "2  p119046  81.000000  97.500000  36.662368  163.500000  119.000000   \n",
       "3  p119046  83.000000  98.000000  36.500000  158.000000  117.000000   \n",
       "4  p119046  84.000000  97.000000  36.651706  154.000000  113.000000   \n",
       "\n",
       "         DBP       Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0  67.642071  18.833348  131.356454  60.0       1    1.0    0.0        -7.79   \n",
       "1  94.000000  22.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "2  90.000000  24.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "3  89.000000  22.500000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "4  85.000000  20.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "\n",
       "   ICULOS  SepsisLabel  hours2sepsis  \n",
       "0     1.0            0           500  \n",
       "1     2.0            0           500  \n",
       "2     3.0            0           500  \n",
       "3     4.0            0           500  \n",
       "4     5.0            0           500  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_full = pd.read_csv('./Data/fully_imputed.csv')\n",
    "sepsis_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p119052</td>\n",
       "      <td>96.151009</td>\n",
       "      <td>97.358717</td>\n",
       "      <td>36.808508</td>\n",
       "      <td>123.139873</td>\n",
       "      <td>87.448856</td>\n",
       "      <td>69.750650</td>\n",
       "      <td>19.305982</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p119052</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.59</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p119052</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>37.100000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.59</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p119052</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p119052</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>116.256093</td>\n",
       "      <td>85.367087</td>\n",
       "      <td>70.655267</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.59</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_id          HR      O2Sat       Temp         SBP        MAP  \\\n",
       "0  p119052   96.151009  97.358717  36.808508  123.139873  87.448856   \n",
       "1  p119052  126.000000  96.000000  36.800000  100.000000  83.000000   \n",
       "2  p119052  126.000000  97.000000  37.100000   93.000000  74.000000   \n",
       "3  p119052  126.000000  99.000000  37.000000   99.000000  85.000000   \n",
       "4  p119052  128.000000  98.000000  36.800000  116.256093  85.367087   \n",
       "\n",
       "         DBP       Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0  69.750650  19.305982  131.356454  43.0       0    1.0    0.0      -125.59   \n",
       "1  65.000000  25.000000  159.000000  43.0       0    1.0    0.0      -125.59   \n",
       "2  63.000000  24.000000  159.000000  43.0       0    1.0    0.0      -125.59   \n",
       "3  65.000000  22.000000  159.000000  43.0       0    1.0    0.0      -125.59   \n",
       "4  70.655267  24.000000  159.000000  43.0       0    1.0    0.0      -125.59   \n",
       "\n",
       "   ICULOS  SepsisLabel  hours2sepsis  \n",
       "0     1.0            0           177  \n",
       "1     2.0            0           176  \n",
       "2     3.0            0           175  \n",
       "3     4.0            0           174  \n",
       "4     5.0            0           173  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train 80% test 20%  split\n",
    "train_sepis_df = sepsis_full[sepsis_full['pat_id'].isin(train_sepsis)]\n",
    "train_nosepis_df = sepsis_full[sepsis_full['pat_id'].isin(train_nosepsis)]\n",
    "test_set_df = sepsis_full[sepsis_full['pat_id'].isin(test_set)]\n",
    "train_set_df = pd.concat([train_sepis_df, train_nosepis_df], ignore_index=True)\n",
    "train_set_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = test_set_df.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p110704</td>\n",
       "      <td>107.794022</td>\n",
       "      <td>96.823416</td>\n",
       "      <td>37.104062</td>\n",
       "      <td>121.632393</td>\n",
       "      <td>84.617518</td>\n",
       "      <td>67.351802</td>\n",
       "      <td>21.384109</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p110704</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>36.708813</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p110704</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>36.400000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p110704</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>36.846801</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p110704</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>36.758329</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_id          HR      O2Sat       Temp         SBP         MAP  \\\n",
       "0  p110704  107.794022  96.823416  37.104062  121.632393   84.617518   \n",
       "1  p110704  101.000000  95.000000  36.708813  135.000000  104.000000   \n",
       "2  p110704  110.000000  94.000000  36.400000  164.000000  135.000000   \n",
       "3  p110704  104.000000  96.000000  36.846801  129.000000   97.000000   \n",
       "4  p110704   94.000000  96.000000  36.758329  144.000000  106.000000   \n",
       "\n",
       "          DBP       Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0   67.351802  21.384109  131.356454  63.0       1    1.0    0.0       -84.29   \n",
       "1   90.000000  11.000000   80.000000  63.0       1    1.0    0.0       -84.29   \n",
       "2  102.000000  10.000000   80.000000  63.0       1    1.0    0.0       -84.29   \n",
       "3   83.000000  14.000000   79.000000  63.0       1    1.0    0.0       -84.29   \n",
       "4   89.000000  16.000000   79.000000  63.0       1    1.0    0.0       -84.29   \n",
       "\n",
       "   ICULOS  SepsisLabel  hours2sepsis  \n",
       "0     1.0            1             0  \n",
       "1     2.0            1             0  \n",
       "2     3.0            1             0  \n",
       "3     4.0            1             0  \n",
       "4     5.0            1             0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vhYz613WvY4u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "# Whether we save dictionary for plotting rolling cov or width\n",
    "save_dict_rolling = True\n",
    "non_stat_solar = True  # Whether X_t in solar contains time covariate\n",
    "if non_stat_solar == False:\n",
    "    # We only visualize rolling results under non-stationary feature X_t\n",
    "    save_dict_rolling = False\n",
    "# train_ls = [0.5, 0.6, 0.7, 0.8]\n",
    "train_ls = [0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h82M4AKEvcOb"
   },
   "source": [
    "## EnbPI & SPCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jNWExRaNvjAF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Train frac at 0.8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prediction_interval_with_SPCI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21084\\3547049350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         EnbPI = prediction_interval_with_SPCI(\n\u001b[0m\u001b[0;32m     64\u001b[0m             X_train, X_predict, Y_train, Y_predict, fit_func=fit_func)\n\u001b[0;32m     65\u001b[0m         \u001b[0mEnbPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_bootstrap_models_online\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_sigmaX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_sigmaX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction_interval_with_SPCI' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Arguments:\n",
    "        conditions: Contain these three below:\n",
    "            use_quantile_regr: bool. True use `quantile_regr`. False use empirical quatile\n",
    "            quantile_regr: str. Which quantile regression to fit residuals (e.g., \"RF\", \"LR\")\n",
    "            fit_func: None or sklearn module with methods `.fit` & `.predict`. If None, use MLP above\n",
    "\n",
    "        fit_sigmaX: bool. True if to fit heteroskedastic errors. ONLY activated if fit_func is NONE (i.e. MLP), because errors are unobserved so `.fit()` does not work\n",
    "\n",
    "        smallT: bool. True if empirical quantile uses not ALL T residual in the past to get quantile (should be tuned as sometimes longer memory causes poor coverage)\n",
    "            past_window: int. If smallT True, EnbPI uses `past_window` most residuals to get width. FOR quantile_regr of residuals, it determines the dimension of the \"feature\" that predict new quantile of residuals autoregressively\n",
    "\n",
    "    Results:\n",
    "        dict: contains dictionary of coverage and width under different training fraction (fix alpha) under various argument combinations\n",
    "\n",
    "'''\n",
    "importlib.reload(sys.modules['utils_quick'])\n",
    "importlib.reload(sys.modules['utils_latest'])\n",
    "dict_full = {}\n",
    "dict_rolling = {}\n",
    "''' Test '''\n",
    "# Conditions: [if simulation, if use quantile regr, which quantile regr]\n",
    "\n",
    "# use quantile residuals: SPCI\n",
    "#use empirical residuals: EnbPI. Naive empirical quantile\n",
    "conditions_real = [[False, ''], [True, 'RF']]\n",
    "# For burn-in period plot\n",
    "PIs_EnbPI = 0\n",
    "PIs_SPCI = 0\n",
    "                   \n",
    "for condition in conditions_real:\n",
    "    use_quantile_regr, quantile_regr = condition\n",
    "    name = 'SPCI' if use_quantile_regr else 'EnbPI'\n",
    "    result_cov, result_width = [], []\n",
    "    for train_frac in train_ls:\n",
    "        print('########################################')\n",
    "        print(f'Train frac at {train_frac}')              \n",
    "        # data_name = 'solar'  # 'electric' or 'solar'\n",
    "        data_name = 'physionet_sepsis'\n",
    "        if data_name == 'physionet_sepsis':\n",
    "            # sepsisdata = pd.read_csv('./Data/fully_imputed.csv')\n",
    "            # sepsisdata = sepsisdata[0:30000]\n",
    "            Y_full = sepsisdata['hours2sepsis']\n",
    "            X_full = sepsisdata.drop(columns=['pat_id','hours2sepsis'])\n",
    "            \n",
    "            fit_func = RandomForestRegressor(n_estimators=10, max_depth=1, criterion='mse',\n",
    "                                                  bootstrap=False, n_jobs=-1)\n",
    "            past_window = 300                 \n",
    "                   \n",
    "        # Y_full = Y_full.to_numpy()  \n",
    "        # X_full = X_full.to_numpy()\n",
    "        # Y_full, X_full = torch.from_numpy(Y_full).float().to(\n",
    "        #         device), torch.from_numpy(X_full).float().to(device)\n",
    "        fit_sigmaX = False\n",
    "        B = 25   #no. of boots          \n",
    "                   \n",
    "        alpha, itrial = 0.1, 0\n",
    "        # N = int(X_full.shape[0]*train_frac)\n",
    "        # X_train, X_predict, Y_train, Y_predict = X_full[:\n",
    "        #                                                 N], X_full[N:], Y_full[:N], Y_full[N:]\n",
    "        X_train = train_set_df.drop(columns=['pat_id','hours2sepsis'])\n",
    "        Y_train = train_set_df['hours2sepsis']\n",
    "        X_predict = test_set_df.drop(columns=['pat_id','hours2sepsis'])\n",
    "        Y_predict = test_set_df['hours2sepsis']\n",
    "\n",
    "        X_train = X_train.to_numpy()\n",
    "        Y_train = Y_train.to_numpy()\n",
    "        X_predict = X_predict.to_numpy()\n",
    "        Y_predict = Y_predict.to_numpy()\n",
    "        Y_train, X_train = torch.from_numpy(Y_train).float().to(device), torch.from_numpy(X_train).float().to(device)\n",
    "        Y_predict, X_predict = torch.from_numpy(Y_predict).float().to(device), torch.from_numpy(X_predict).float().to(device)\n",
    "\n",
    "        # Train\n",
    "        EnbPI = prediction_interval_with_SPCI(\n",
    "            X_train, X_predict, Y_train, Y_predict, fit_func=fit_func)\n",
    "        EnbPI.fit_bootstrap_models_online(B, fit_sigmaX=fit_sigmaX)\n",
    "        # Under cond quantile, we are ALREADY using the last window for prediction so smallT is True\n",
    "        smallT = not use_quantile_regr\n",
    "        EnbPI.compute_PIs_Ensemble_online(\n",
    "            alpha, smallT=smallT, past_window=past_window, use_quantile_regr=use_quantile_regr,\n",
    "            quantile_regr=quantile_regr)\n",
    "        results = EnbPI.get_results(alpha, data_name, itrial)\n",
    "        result_cov.append(results['coverage'].item())\n",
    "        result_width.append(results['width'].item())\n",
    "        # Lastly, compute rolling width to plot\n",
    "        PI = EnbPI.PIs_Ensemble\n",
    "        if use_quantile_regr:\n",
    "            PIs_SPCI = PI\n",
    "        else:\n",
    "            PIs_EnbPI = PI\n",
    "        Ytest = EnbPI.Y_predict.cpu().detach().numpy()\n",
    "        coverage = ((np.array(PI['lower']) <= Ytest)\n",
    "                    & (np.array(PI['upper']) >= Ytest))\n",
    "        width = ((np.array(PI['upper'])-np.array(PI['lower'])))\n",
    "        # cov_moving = rolling_avg(coverage)\n",
    "        # width_moving = rolling_avg(width)\n",
    "        dict_rolling[f'{name}{np.round(train_frac,2)}'] = np.vstack([\n",
    "            coverage, width])\n",
    "        if save_dict_rolling:\n",
    "            with open(f'EnbPI_rolling_{data_name}.p', 'wb') as fp:\n",
    "                pickle.dump(dict_rolling, fp,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    dict_full[name] = np.vstack([result_cov, result_width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPWsYs6Dz8UT"
   },
   "source": [
    "Due to variability, what you see for EnbPI may be different. However, in general EnbPI intervals are wider\n",
    "\n",
    "0.8 indicates that training fraction as a percentage of total data is 80\\% or 1600 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v6yJdKh8wWxY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       0.8\n",
      "EnbPI coverage    0.918333\n",
      "EnbPI width     185.464391\n",
      "SPCI coverage     0.975167\n",
      "SPCI width       21.572713\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "{} &     0.8 \\\\\n",
      "\\midrule\n",
      "EnbPI coverage &    0.92 \\\\\n",
      "EnbPI width    &  185.46 \\\\\n",
      "SPCI coverage  &    0.98 \\\\\n",
      "SPCI width     &   21.57 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_to_latex(dict_full)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
