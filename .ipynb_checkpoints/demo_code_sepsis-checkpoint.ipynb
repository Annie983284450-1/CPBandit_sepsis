{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xysGqjV0p5aE"
   },
   "source": [
    "This file demonstrate the solar experiment shown in section 6.3, where we compare EnbPI, SPCI, AdaptiveCI, and Nex-CP WLS\n",
    "\n",
    "It is current written to be executed in google colaboratory.\n",
    "\n",
    "Please place the following files in the same directory as this `.ipynb`, which are a part of this Github repository\n",
    "\n",
    "```\n",
    "  utils_quick.py\n",
    "\n",
    "  utils_latest.py\n",
    "\n",
    "  utils_EnbPI.py\n",
    "\n",
    "  PI_class_EnbPI.py\n",
    "\n",
    "  Data/Solar_Atl_data.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQL2E4phhx9P"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64286,
     "status": "ok",
     "timestamp": 1683233051066,
     "user": {
      "displayName": "anni zhou",
      "userId": "09983559370256030799"
     },
     "user_tz": 240
    },
    "id": "CXt0Szm9q_Xx",
    "outputId": "c3f0f983-1ccf-4d31-f080-a1e1840fb556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=256418 sha256=a72c0f1fc7d7a5514bb639a3337900e67f385bd3266153a8966ed7fd9cc865a7\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.13.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting scikit-garden\n",
      "  Downloading scikit-garden-0.1.3.tar.gz (317 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-garden) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-garden) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from scikit-garden) (1.2.2)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from scikit-garden) (0.29.34)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-garden) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-garden) (1.2.0)\n",
      "Building wheels for collected packages: scikit-garden\n",
      "  Building wheel for scikit-garden (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for scikit-garden: filename=scikit_garden-0.1.3-cp310-cp310-linux_x86_64.whl size=878653 sha256=2561a4b72618cdd3bd9dff86f7e9eab642af138a07a6ba816c7aa0acd611fe16\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/14/5d/bd2f7a72e4542d8547b61fca6d328d18ac2b5c0c37faf1c342\n",
      "Successfully built scikit-garden\n",
      "Installing collected packages: scikit-garden\n",
      "Successfully installed scikit-garden-0.1.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn==0.22.2 (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21.1, 0.21.2, 0.21.3, 0.22, 0.22.1, 0.22.2.post1, 0.23.0, 0.23.1, 0.23.2, 0.24.0, 0.24.1, 0.24.2, 1.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.2.0rc1, 1.2.0, 1.2.1, 1.2.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn==0.22.2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pickle5\n",
    "!pip install --upgrade --no-deps statsmodels\n",
    "!pip install scikit-garden\n",
    "!pip install scikit-learn==0.22.2 # Needed for scikit-garden to work properly. Restart runtime is NOT needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kP22ZrKc5XB"
   },
   "outputs": [],
   "source": [
    "import skgarden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4DWIPYmcqY5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# path = '/content/drive/MyDrive/EnbPI' # You are free to replace this with where you place this file on drive\n",
    "# sys.path.insert(0,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bloW_QoiZNE"
   },
   "outputs": [],
   "source": [
    "# %cd /content/dr/ive/MyDrive/EnbPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3wj3Ngsc5XE"
   },
   "outputs": [],
   "source": [
    "# MPS acceleration is available on MacOS 12.3+\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWSzk0dNQf8W"
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import importlib as ipb\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "import math\n",
    "import time as time\n",
    "import utils_quick as util\n",
    "import utils_latest\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from skgarden import RandomForestQuantileRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjuucWAivJOC"
   },
   "source": [
    "# Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2blvGaelQXAO"
   },
   "outputs": [],
   "source": [
    "#### Main Class ####\n",
    "\n",
    "# The same class as \n",
    "# class prediction_interval() in tests_paper+supp_journal.py\n",
    "class prediction_interval_with_SPCI():\n",
    "    '''\n",
    "        Create prediction intervals assuming Y_t = f(X_t) + \\sigma(X_t)\\eps_t\n",
    "        Currently, assume the regression function is by default MLP implemented \n",
    "        with PyTorch, as it needs to estimate BOTH f(X_t) and \\sigma(X_t), \n",
    "        where the latter is impossible to estimate using scikit-learn modules\n",
    "        Most things carry out, except that we need to have different estimators \n",
    "        for f and \\sigma.\n",
    "        fit_func = None: use MLP above\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X_train, X_predict, Y_train, Y_predict, fit_func=None):\n",
    "        self.regressor = fit_func\n",
    "        self.X_train = X_train\n",
    "        self.X_predict = X_predict\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_predict = Y_predict\n",
    "        # Predicted training data centers by EnbPI\n",
    "        self.Ensemble_train_interval_centers = []\n",
    "        self.Ensemble_train_interval_sigma = []\n",
    "        # Predicted test data centers by EnbPI\n",
    "        self.Ensemble_pred_interval_centers = []\n",
    "        self.Ensemble_pred_interval_sigma = []\n",
    "        self.Ensemble_online_resid = []  # LOO scores\n",
    "        self.beta_hat_bins = []\n",
    "\n",
    "    def fit_bootstrap_models_online(self, B, miss_test_idx=[], fit_sigmaX=True):\n",
    "        '''\n",
    "          Train B bootstrap estimators from subsets of (X_train, Y_train), \n",
    "          compute aggregated predictors, and compute the residuals\n",
    "          fit_sigmaX: If False, just avoid predicting \\sigma(X_t) by defaulting it to 1\n",
    "        '''\n",
    "        n, d = self.X_train.shape\n",
    "        n1 = len(self.X_predict)\n",
    "        # hold indices of training data for each f^b\n",
    "        boot_samples_idx = util.generate_bootstrap_samples(n, n, B)\n",
    "        # hold predictions from each f^b for fX and sigma&b for sigma\n",
    "        boot_predictionsFX = torch.zeros(B, n+n1).to(device)\n",
    "        boot_predictionsSigmaX = torch.ones(B, n+n1).to(device)\n",
    "        # for i^th column, it shows which f^b uses i in training (so exclude in aggregation)\n",
    "        in_boot_sample = np.zeros((B, n), dtype=bool)\n",
    "        out_sample_predictFX = torch.zeros(n, n1).to(device)\n",
    "        out_sample_predictSigmaX = torch.ones(n, n1).to(device)\n",
    "        start = time.time()\n",
    "        Xfull = torch.vstack([self.X_train, self.X_predict])\n",
    "        for b in range(B):\n",
    "            Xboot, Yboot = self.X_train[boot_samples_idx[b],\n",
    "                                        :], self.Y_train[boot_samples_idx[b], ]\n",
    "            in_boot_sample[b, boot_samples_idx[b]] = True\n",
    "            if self.regressor.__class__.__name__ == 'NoneType':\n",
    "                start1 = time.time()\n",
    "                model_f = MLP(d).to(device)\n",
    "                optimizer_f = torch.optim.Adam(model_f.parameters(), lr=1e-3)\n",
    "                if fit_sigmaX:\n",
    "                    model_sigma = MLP(d, sigma=True).to(device)\n",
    "                    optimizer_sigma = torch.optim.Adam(\n",
    "                        model_sigma.parameters(), lr=2e-3)\n",
    "                for epoch in range(300):\n",
    "                    fXhat = model_f(Xboot)\n",
    "                    sigmaXhat = torch.ones(len(fXhat)).to(device)\n",
    "                    if fit_sigmaX:\n",
    "                        sigmaXhat = model_sigma(Xboot)\n",
    "                    loss = ((Yboot-fXhat)/sigmaXhat).pow(2).mean()/2\n",
    "                    optimizer_f.zero_grad()\n",
    "                    if fit_sigmaX:\n",
    "                        optimizer_sigma.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_f.step()\n",
    "                    if fit_sigmaX:\n",
    "                        optimizer_sigma.step()\n",
    "                with torch.no_grad():\n",
    "                    boot_predictionsFX[b] = model_f(Xfull).flatten()\n",
    "                    if fit_sigmaX:\n",
    "                        boot_predictionsSigmaX[b] = model_sigma(\n",
    "                            Xfull).flatten()\n",
    "                print(\n",
    "                    f'Took {time.time()-start1} secs to finish the {b}th boostrap model')\n",
    "            else:\n",
    "                model = self.regressor\n",
    "                model.fit(Xboot, Yboot)\n",
    "                boot_predictionsFX[b] = torch.from_numpy(\n",
    "                    model.predict(Xfull).flatten()).to(device)\n",
    "                # NOTE, NO sigma estimation because these methods by deFAULT are fitting Y, but we have no observation of errors\n",
    "        print(\n",
    "            f'Finish Fitting {B} Bootstrap models, took {time.time()-start} secs.')\n",
    "        start = time.time()\n",
    "        keep = []\n",
    "        for i in range(n):\n",
    "            b_keep = np.argwhere(~(in_boot_sample[:, i])).reshape(-1)\n",
    "            pred_iFX = boot_predictionsFX[b_keep, i].mean()\n",
    "            pred_iSigmaX = boot_predictionsSigmaX[b_keep, i].mean()\n",
    "            pred_testFX = boot_predictionsFX[b_keep, n:].mean(0)\n",
    "            pred_testSigmaX = boot_predictionsSigmaX[b_keep, n:].mean(0)\n",
    "            if(len(b_keep) > 0):\n",
    "                self.Ensemble_train_interval_centers.append(pred_iFX)\n",
    "                self.Ensemble_train_interval_sigma.append(pred_iSigmaX)\n",
    "                resid_LOO = (self.Y_train[i] - pred_iFX)/pred_iSigmaX\n",
    "                out_sample_predictFX[i] = pred_testFX\n",
    "                out_sample_predictSigmaX[i] = pred_testSigmaX\n",
    "                keep = keep+[b_keep]\n",
    "            self.Ensemble_online_resid.append(resid_LOO.item())\n",
    "        sorted_out_sample_predictFX = out_sample_predictFX.mean(0)  # length n1\n",
    "        sorted_out_sample_predictSigmaX = out_sample_predictSigmaX.mean(\n",
    "            0)  # length n1\n",
    "        resid_out_sample = (\n",
    "            self.Y_predict-sorted_out_sample_predictFX)/sorted_out_sample_predictSigmaX\n",
    "        if len(miss_test_idx) > 0:\n",
    "            # Replace missing residuals with that from the immediate predecessor that is not missing, as\n",
    "            # o/w we are not assuming prediction data are missing\n",
    "            for idx in range(len(miss_test_idx)):\n",
    "                i = miss_test_idx[idx]\n",
    "                if i > 0:\n",
    "                    j = i-1\n",
    "                    while j in miss_test_idx[:idx]:\n",
    "                        j -= 1\n",
    "                    resid_out_sample[i] = resid_out_sample[j]\n",
    "\n",
    "                else:\n",
    "                    # The first Y during testing is missing, let it be the last of the training residuals\n",
    "                    # note, training data already takes out missing values, so doing is is fine\n",
    "                    resid_out_sample[0] = self.Ensemble_online_resid[-1]\n",
    "        self.Ensemble_online_resid = np.append(\n",
    "            self.Ensemble_online_resid, resid_out_sample.cpu().detach().numpy())\n",
    "        # print(f'Finish Computing LOO residuals, took {time.time()-start} secs.')\n",
    "        # print(f'Max LOO test residual is {np.max(self.Ensemble_online_resid[n:])}')\n",
    "        # print(f'Min LOO test residual is {np.min(self.Ensemble_online_resid[n:])}')\n",
    "        self.Ensemble_pred_interval_centers = sorted_out_sample_predictFX\n",
    "        self.Ensemble_pred_interval_sigma = sorted_out_sample_predictSigmaX\n",
    "\n",
    "    def compute_PIs_Ensemble_online(self, alpha, stride=1, smallT=True, past_window=100, use_quantile_regr=False, quantile_regr='RF'):\n",
    "        '''\n",
    "            smallT: if True, we would only start with the last n number of LOO residuals, rather than use the full length T ones. Used in change detection\n",
    "                NOTE: smallT can be important if time-series is very dynamic, in which case training MORE data may actaully be worse (because quantile longer)\n",
    "                HOWEVER, if fit quantile regression, set it to be FALSE because we want to have many training pts for the quantile regressor\n",
    "            use_quantile_regr: if True, we fit conditional quantile to compute the widths, rather than simply using empirical quantile\n",
    "        '''\n",
    "        n1 = len(self.X_train)\n",
    "        if smallT:\n",
    "            n1 = min(past_window, len(self.X_train))\n",
    "        # Now f^b and LOO residuals have been constructed from earlier\n",
    "        out_sample_predict = self.Ensemble_pred_interval_centers.cpu().detach().numpy()\n",
    "        out_sample_predictSigmaX = self.Ensemble_pred_interval_sigma\n",
    "        start = time.time()\n",
    "        # Matrix, where each row is a UNIQUE slice of residuals with length stride.\n",
    "        resid_strided = util.strided_app(\n",
    "            self.Ensemble_online_resid[len(self.X_train)-n1:-1], n1, stride)\n",
    "        print(f'Shape of slided residual lists is {resid_strided.shape}')\n",
    "        num_unique_resid = resid_strided.shape[0]\n",
    "        width_left = np.zeros(num_unique_resid)\n",
    "        width_right = np.zeros(num_unique_resid)\n",
    "        # # NEW, alpha becomes alpha_t. Uncomment things below if we decide to use this upgraded EnbPI\n",
    "        # alpha_t = alpha\n",
    "        # errs = []\n",
    "        # gamma = 0.005\n",
    "        # method = 'simple'  # 'simple' or 'complex'\n",
    "        # self.alphas = []\n",
    "        # NOTE: 'max_features='log2', max_depth=2' make the model \"simpler\", which improves performance in practice\n",
    "        for i in range(num_unique_resid):\n",
    "            # for p in range(stride):  # NEW for adaptive alpha\n",
    "            past_resid = resid_strided[i, :]\n",
    "            curr_SigmaX = out_sample_predictSigmaX[i].item()\n",
    "            if use_quantile_regr:\n",
    "                # New predicted conditional quntile\n",
    "                # 1. Get \"past_resid\" into an auto-regressive fashion\n",
    "                # This should be more carefully examined, b/c it depends on how long \\hat{\\eps}_t depends on the past\n",
    "                # From practice, making it small make intervals wider\n",
    "                n2 = past_window\n",
    "                residX = sliding_window_view(past_resid, window_shape=n2)\n",
    "                residY = past_resid[n2:]\n",
    "                # 2. Fit the model. Default quantile regressor is the quantile RF from\n",
    "                # scikit-garden: https://scikit-garden.github.io/\n",
    "                # NOTE, should NOT warm start, as it makes result poor, although training is longer\n",
    "                if quantile_regr == 'RF':\n",
    "                    rfqr = RandomForestQuantileRegressor(\n",
    "                        max_depth=2, random_state=0)\n",
    "                    rfqr.fit(residX[:-1], residY)\n",
    "                    # 3. Find best \\hat{\\beta} via evaluating many quantiles\n",
    "                    beta_hat_bin = util.binning_use_RF_quantile_regr(\n",
    "                        rfqr, residX[-1], alpha)\n",
    "                    width_left[i] = curr_SigmaX*rfqr.predict(\n",
    "                        residX[-1].reshape(1, -1), math.ceil(100 * beta_hat_bin))\n",
    "                    width_right[i] = curr_SigmaX*rfqr.predict(\n",
    "                        residX[-1].reshape(1, -1), math.ceil(100 * (1-alpha+beta_hat_bin)))\n",
    "                # if quantile_regr == 'LR':\n",
    "                #     start1 = time.time()\n",
    "                #     wleft, wright = util.binning_use_linear_quantile_regr(\n",
    "                #         residX, residY, alpha)\n",
    "                #     if i == 0:\n",
    "                #         print(\n",
    "                #             f'100 Linear QRegr approx. takes {100*(time.time()-start1)} secs.')\n",
    "                #     width_left[i] = curr_SigmaX*wleft\n",
    "                #     width_right[i] = curr_SigmaX*wright\n",
    "                if i % int(num_unique_resid/20) == 0:\n",
    "                    print(\n",
    "                        f'Width at test {i} is {width_right[i]-width_left[i]}')\n",
    "            else:\n",
    "                # Naive empirical quantile\n",
    "                # The number of bins will be determined INSIDE binning\n",
    "                beta_hat_bin = util.binning(past_resid, alpha)\n",
    "                # beta_hat_bin = util.binning(past_resid, alpha_t)\n",
    "                self.beta_hat_bins.append(beta_hat_bin)\n",
    "                width_left[i] = curr_SigmaX*np.percentile(\n",
    "                    past_resid, math.ceil(100*beta_hat_bin))\n",
    "                width_right[i] = curr_SigmaX*np.percentile(\n",
    "                    past_resid, math.ceil(100*(1-alpha+beta_hat_bin)))\n",
    "        print(\n",
    "            f'Finish Computing {num_unique_resid} UNIQUE Prediction Intervals, took {time.time()-start} secs.')\n",
    "        # This is because |width|=T1/stride.\n",
    "        width_left = np.repeat(width_left, stride)\n",
    "        # This is because |width|=T1/stride.\n",
    "        width_right = np.repeat(width_right, stride)\n",
    "        PIs_Ensemble = pd.DataFrame(np.c_[out_sample_predict+width_left,\n",
    "                                          out_sample_predict+width_right], columns=['lower', 'upper'])\n",
    "        self.PIs_Ensemble = PIs_Ensemble\n",
    "\n",
    "    '''\n",
    "        All together\n",
    "    '''\n",
    "\n",
    "    def get_results(self, alpha, data_name, itrial, true_Y_predict=[], method='Ensemble'):\n",
    "        '''\n",
    "            NOTE: I added a \"true_Y_predict\" option, which will be used for calibrating coverage under missing data\n",
    "            In particular, this is needed when the Y_predict we use for training is NOT the same as true Y_predict\n",
    "        '''\n",
    "        results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',\n",
    "                                        'method', 'train_size', 'coverage', 'width'])\n",
    "        train_size = len(self.X_train)\n",
    "        if method == 'Ensemble':\n",
    "            PI = self.PIs_Ensemble\n",
    "        Ytest = self.Y_predict.cpu().detach().numpy()\n",
    "        coverage = ((np.array(PI['lower']) <= Ytest) & (\n",
    "            np.array(PI['upper']) >= Ytest)).mean()\n",
    "        if len(true_Y_predict) > 0:\n",
    "            coverage = ((np.array(PI['lower']) <= true_Y_predict) & (\n",
    "                np.array(PI['upper']) >= true_Y_predict)).mean()\n",
    "        print(f'Average Coverage is {coverage}')\n",
    "        width = (PI['upper'] - PI['lower']).mean()\n",
    "        print(f'Average Width is {width}')\n",
    "        results.loc[len(results)] = [itrial, data_name,\n",
    "                                     'torch_MLP', method, train_size, coverage, width]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTNEKRP8c5XJ"
   },
   "outputs": [],
   "source": [
    "def CP_LS(X, Y, x, alpha, weights=[], tags=[]):\n",
    "    # Barber et al. 2022: Nex-CP\n",
    "    # weights are used for computing quantiles for the prediction interval\n",
    "    # tags are used as weights in weighted least squares regression\n",
    "    n = len(Y)\n",
    "\n",
    "    if(len(tags) == 0):\n",
    "        tags = np.ones(n+1)\n",
    "\n",
    "    if(len(weights) == 0):\n",
    "        weights = np.ones(n+1)\n",
    "    if(len(weights) == n):\n",
    "        weights = np.r_[weights, 1]\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # randomly permute one weight for the regression\n",
    "    random_ind = int(np.where(np.random.multinomial(1, weights, 1))[1])\n",
    "    tags[np.c_[random_ind, n]] = tags[np.c_[n, random_ind]]\n",
    "\n",
    "    XtX = (X.T*tags[:-1]).dot(X) + np.outer(x, x)*tags[-1]\n",
    "    a = Y - X.dot(np.linalg.solve(XtX, (X.T*tags[:-1]).dot(Y)))\n",
    "    b = -X.dot(np.linalg.solve(XtX, x))*tags[-1]\n",
    "    a1 = -x.T.dot(np.linalg.solve(XtX, (X.T*tags[:-1]).dot(Y)))\n",
    "    b1 = 1 - x.T.dot(np.linalg.solve(XtX, x))*tags[-1]\n",
    "    # if we run weighted least squares on (X[1,],Y[1]),...(X[n,],Y[n]),(x,y)\n",
    "    # then a + b*y = residuals of data points 1,..,n\n",
    "    # and a1 + b1*y = residual of data point n+1\n",
    "\n",
    "    y_knots = np.sort(\n",
    "        np.unique(np.r_[((a-a1)/(b1-b))[b1-b != 0], ((-a-a1)/(b1+b))[b1+b != 0]]))\n",
    "    y_inds_keep = np.where(((np.abs(np.outer(a1+b1*y_knots, np.ones(n)))\n",
    "                             > np.abs(np.outer(np.ones(len(y_knots)), a)+np.outer(y_knots, b))) *\n",
    "                            weights[:-1]).sum(1) <= 1-alpha)[0]\n",
    "    y_PI = np.array([y_knots[y_inds_keep.min()], y_knots[y_inds_keep.max()]])\n",
    "    if(weights[:-1].sum() <= 1-alpha):\n",
    "        y_PI = np.array([-np.inf, np.inf])\n",
    "    return y_PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5UEfyuIc5XK"
   },
   "outputs": [],
   "source": [
    "#### Model and data helper ####\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d, sigma=False):\n",
    "        super(MLP, self).__init__()\n",
    "        H = 64\n",
    "        layers = [nn.Linear(d, H), nn.ReLU(), nn.Linear(\n",
    "            H, H), nn.ReLU(), nn.Linear(H, 1)]\n",
    "        self.sigma = sigma\n",
    "        if self.sigma:\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        perturb = 1e-3 if self.sigma else 0\n",
    "        return self.layers(x)+perturb\n",
    "\n",
    "\n",
    "def get_new_data():\n",
    "    ''' Note, the difference from earlier case 3 in paper is that\n",
    "        1) I reduce d from 100 to 20,\n",
    "        2) I let X to be different, so sigmaX differs\n",
    "            The sigmaX is a linear model so this effect in X is immediate\n",
    "        I keep the same AR(1) eps & everything else.'''\n",
    "    def True_mod_nonlinear_pre(feature):\n",
    "        '''\n",
    "        Input:\n",
    "        Output:\n",
    "        Description:\n",
    "            f(feature): R^d -> R\n",
    "        '''\n",
    "        # Attempt 3 Nonlinear model:\n",
    "        # f(X)=sqrt(1+(beta^TX)+(beta^TX)^2+(beta^TX)^3), where 1 is added in case beta^TX is zero\n",
    "        d = len(feature)\n",
    "        np.random.seed(0)\n",
    "        # e.g. 20% of the entries are NON-missing\n",
    "        beta1 = random(1, d, density=0.2).A\n",
    "        betaX = np.abs(beta1.dot(feature))\n",
    "        return (betaX + betaX**2 + betaX**3)**(1/4)\n",
    "    Tot, d = 2000, 20\n",
    "    Fmap = True_mod_nonlinear_pre\n",
    "    # Multiply each random feature by exponential component, which is repeated every Tot/rep elements\n",
    "    rep = 10\n",
    "    mult = np.exp(np.repeat(np.linspace(0, 2, rep), Tot/rep)).reshape(Tot, 1)\n",
    "    X = np.random.rand(Tot, d)*mult\n",
    "    fX = np.array([Fmap(x) for x in X]).flatten()\n",
    "    beta_Sigma = 0.1*np.ones(d)\n",
    "    sigmaX = np.maximum(X.dot(beta_Sigma).T, 0)\n",
    "    with open(f'Data_nochangepts_nonlinear.p', 'rb') as fp:\n",
    "        Data_dc = pickle.load(fp)\n",
    "    eps = Data_dc['Eps']\n",
    "    Y = fX + sigmaX*eps\n",
    "    np.random.seed(1103)\n",
    "    idx = np.random.choice(Tot, Tot, replace=False)\n",
    "    Y, X, fX, sigmaX, eps = Y[idx], X[idx], fX[idx], sigmaX[idx], eps[idx]\n",
    "    return {'Y': torch.from_numpy(Y).float(), 'X': torch.from_numpy(X).float(), 'f(X)': fX, 'sigma(X)': sigmaX, 'Eps': eps}\n",
    "\n",
    "\n",
    "def get_new_data_simple(num_pts, alpha, beta):\n",
    "    '''\n",
    "        Y_t = alpha*Y_{t-1}+\\eps_t\n",
    "        \\eps_t = beta*\\eps_{t-1}+v_t\n",
    "        v_t ~ N(0,1)\n",
    "        So X_t = Y_{t-1}, f(X_t) = alpha*X_t\n",
    "        If t = 0:\n",
    "            X_t = 0, Y_t=\\eps_t = v_t\n",
    "    '''\n",
    "    v0 = torch.randn(1)\n",
    "    Y, X, fX, eps = [v0], [torch.zeros(1)], [torch.zeros(1)], [v0]\n",
    "    scale = torch.sqrt(torch.ones(1)*0.1)\n",
    "    for _ in range(num_pts-1):\n",
    "        vt = torch.randn(1)*scale\n",
    "        X.append(Y[-1])\n",
    "        fX.append(alpha*Y[-1])\n",
    "        eps.append(beta*eps[-1]+vt)\n",
    "        Y.append(fX[-1]+eps[-1])\n",
    "    Y, X, fX, eps = torch.hstack(Y), torch.vstack(\n",
    "        X), torch.vstack(fX), torch.hstack(eps)\n",
    "    return {'Y': Y.float(), 'X': X.float(), 'f(X)': fX, 'Eps': eps}\n",
    "\n",
    "\n",
    "def electric_dataset():\n",
    "    # ELEC2 data set\n",
    "    # downloaded from https://www.kaggle.com/yashsharan/the-elec2-dataset\n",
    "    data = pd.read_csv('electricity-normalized.csv')\n",
    "    col_names = data.columns\n",
    "    data = data.to_numpy()\n",
    "\n",
    "    # remove the first stretch of time where 'transfer' does not vary\n",
    "    data = data[17760:]\n",
    "\n",
    "    # set up variables for the task (predicting 'transfer')\n",
    "    covariate_col = ['nswprice', 'nswdemand', 'vicprice', 'vicdemand']\n",
    "    response_col = 'transfer'\n",
    "    # keep data points for 9:00am - 12:00pm\n",
    "    keep_rows = np.where((data[:, 2] > data[17, 2])\n",
    "                         & (data[:, 2] < data[24, 2]))[0]\n",
    "\n",
    "    X = data[keep_rows][:, np.where(\n",
    "        [t in covariate_col for t in col_names])[0]]\n",
    "    Y = data[keep_rows][:, np.where(col_names == response_col)[0]].flatten()\n",
    "    X = X.astype('float64')\n",
    "    Y = Y.astype('float64')\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "#### Other helpers ####\n",
    "\n",
    "def rolling_avg(x, window=100):\n",
    "    return np.convolve(x, np.ones(window)/window)[(window-1):-window]\n",
    "\n",
    "\n",
    "def dict_to_latex(dict):\n",
    "    DF = pd.DataFrame.from_dict(np.vstack(dict.values()))\n",
    "    keys = list(dict.keys())\n",
    "    index = np.array([[f'{key} coverage', f'{key} width']\n",
    "                     for key in keys]).flatten()\n",
    "    DF.index = index\n",
    "    DF.columns = train_ls\n",
    "    print(DF)\n",
    "    print(DF.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hwUJlqnc5XK"
   },
   "outputs": [],
   "source": [
    "# solar_df = pd.read_csv(\"/Data/Solar_Atl_data.csv\" ) # without the '.', it won't read\n",
    "# solar_df = pd.read_csv(\"./Data/Solar_Atl_data.csv\" )\n",
    "sepsis_df = pd.read_csv(\"./Data/fully_imputed.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3glIWoasc5XL",
    "outputId": "87868bce-0adc-4139-80d7-bd5df7dce8d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>hours2sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.045029</td>\n",
       "      <td>97.403481</td>\n",
       "      <td>36.708390</td>\n",
       "      <td>124.795742</td>\n",
       "      <td>85.718264</td>\n",
       "      <td>67.642071</td>\n",
       "      <td>18.833348</td>\n",
       "      <td>131.356454</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p119046</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.644472</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p119046</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>36.662368</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p119046</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p119046</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>36.651706</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_id         HR      O2Sat       Temp         SBP         MAP  \\\n",
       "0  p119046  84.045029  97.403481  36.708390  124.795742   85.718264   \n",
       "1  p119046  85.000000  98.000000  36.644472  168.000000  123.000000   \n",
       "2  p119046  81.000000  97.500000  36.662368  163.500000  119.000000   \n",
       "3  p119046  83.000000  98.000000  36.500000  158.000000  117.000000   \n",
       "4  p119046  84.000000  97.000000  36.651706  154.000000  113.000000   \n",
       "\n",
       "         DBP       Resp     Glucose   Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0  67.642071  18.833348  131.356454  60.0       1    1.0    0.0        -7.79   \n",
       "1  94.000000  22.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "2  90.000000  24.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "3  89.000000  22.500000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "4  85.000000  20.000000   98.000000  60.0       1    1.0    0.0        -7.79   \n",
       "\n",
       "   ICULOS  SepsisLabel  hours2sepsis  \n",
       "0     1.0            0           500  \n",
       "1     2.0            0           500  \n",
       "2     3.0            0           500  \n",
       "3     4.0            0           500  \n",
       "4     5.0            0           500  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg78jm5Tc5XL",
    "outputId": "58991a05-84e4-4a63-8fc1-ae81502c5f45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pat_id          0.0\n",
       "HR              0.0\n",
       "O2Sat           0.0\n",
       "Temp            0.0\n",
       "SBP             0.0\n",
       "MAP             0.0\n",
       "DBP             0.0\n",
       "Resp            0.0\n",
       "Glucose         0.0\n",
       "Age             0.0\n",
       "Gender          0.0\n",
       "Unit1           0.0\n",
       "Unit2           0.0\n",
       "HospAdmTime     0.0\n",
       "ICULOS          0.0\n",
       "SepsisLabel     0.0\n",
       "hours2sepsis    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_df.isnull().sum()/len(solar_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXrLT5QDvMVH"
   },
   "source": [
    "# Running tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhYz613WvY4u"
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "# Whether we save dictionary for plotting rolling cov or width\n",
    "save_dict_rolling = True\n",
    "non_stat_solar = True  # Whether X_t in solar contains time covariate\n",
    "if non_stat_solar == False:\n",
    "    # We only visualize rolling results under non-stationary feature X_t\n",
    "    save_dict_rolling = False\n",
    "train_ls = [0.5, 0.6, 0.7, 0.8]\n",
    "train_ls = [0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h82M4AKEvcOb"
   },
   "source": [
    "## EnbPI & SPCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNWExRaNvjAF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Arguments:\n",
    "        simulation: bool. True use simulated data. False use solar\n",
    "            simul_type: int. 1 = simple state-space. 2 = non-statioanry. 3 = heteroskedastic\n",
    "            The latter 2 follows from case 3 in paper\n",
    "        conditions: Contain these three below:\n",
    "            use_quantile_regr: bool. True use `quantile_regr`. False use empirical quatile\n",
    "            quantile_regr: str. Which quantile regression to fit residuals (e.g., \"RF\", \"LR\")\n",
    "            fit_func: None or sklearn module with methods `.fit` & `.predict`. If None, use MLP above\n",
    "\n",
    "        fit_sigmaX: bool. True if to fit heteroskedastic errors. ONLY activated if fit_func is NONE (i.e. MLP), because errors are unobserved so `.fit()` does not work\n",
    "\n",
    "        smallT: bool. True if empirical quantile uses not ALL T residual in the past to get quantile (should be tuned as sometimes longer memory causes poor coverage)\n",
    "            past_window: int. If smallT True, EnbPI uses `past_window` most residuals to get width. FOR quantile_regr of residuals, it determines the dimension of the \"feature\" that predict new quantile of residuals autoregressively\n",
    "\n",
    "    Results:\n",
    "        dict: contains dictionary of coverage and width under different training fraction (fix alpha) under various argument combinations\n",
    "\n",
    "'''\n",
    "importlib.reload(sys.modules['utils_quick'])\n",
    "importlib.reload(sys.modules['utils_latest'])\n",
    "dict_full = {}\n",
    "dict_rolling = {}\n",
    "''' Test '''\n",
    "# Conditions: [if simulation, if use quantile regr, which quantile regr]\n",
    "# conditions_simulation = [[True, False, ''], [True, True, 'RF']]\n",
    "conditions_real = [[False, ''], [True, 'RF']]\n",
    "# For burn-in period plot\n",
    "PIs_EnbPI = 0\n",
    "PIs_SPCI = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhewBKIic5XO",
    "outputId": "845a65fe-bf71-4eb6-ed9f-b7aaf9f165af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Train frac at 0.8\n",
      "(8760, 8)\n",
      "Finish Fitting 25 Bootstrap models, took 0.7117218971252441 secs.\n",
      "Shape of slided residual lists is (400, 500)\n",
      "Finish Computing 400 UNIQUE Prediction Intervals, took 0.5981330871582031 secs.\n",
      "Average Coverage is 0.885\n",
      "Average Width is 48.88999833350188\n",
      "########################################\n",
      "Train frac at 0.8\n",
      "(8760, 8)\n",
      "Finish Fitting 25 Bootstrap models, took 0.7067630290985107 secs.\n",
      "Shape of slided residual lists is (400, 1600)\n",
      "Width at test 0 is 38.67156078909607\n",
      "Width at test 20 is 37.60465071971953\n",
      "Width at test 40 is 37.9516339438165\n",
      "Width at test 60 is 37.1332026946799\n",
      "Width at test 80 is 45.25657246616814\n",
      "Width at test 100 is 44.11689774103705\n",
      "Width at test 120 is 49.04645113597843\n",
      "Width at test 140 is 38.33048897224109\n",
      "Width at test 160 is 38.31990231060939\n",
      "Width at test 180 is 32.55248493803181\n",
      "Width at test 200 is 58.446171967449885\n",
      "Width at test 220 is 41.69975052126342\n",
      "Width at test 240 is 52.56334880801892\n",
      "Width at test 260 is 32.51728684133373\n",
      "Width at test 280 is 37.03963296404748\n",
      "Width at test 300 is 33.2919276163043\n",
      "Width at test 320 is 45.082753071324646\n",
      "Width at test 340 is 73.61568817683983\n",
      "Width at test 360 is 60.26343711438365\n",
      "Width at test 380 is 38.23632707688535\n",
      "Finish Computing 400 UNIQUE Prediction Intervals, took 146.87307405471802 secs.\n",
      "Average Coverage is 0.9125\n",
      "Average Width is 43.294596985319586\n"
     ]
    }
   ],
   "source": [
    "for condition in conditions_real:\n",
    "    use_quantile_regr, quantile_regr = condition\n",
    "    name = 'SPCI' if use_quantile_regr else 'EnbPI'\n",
    "    result_cov, result_width = [], []\n",
    "    for train_frac in train_ls:\n",
    "        print('########################################')\n",
    "        print(f'Train frac at {train_frac}')\n",
    "        \n",
    "        # if simulation:\n",
    "        #     simul_type = 2  # 1, 2, 3\n",
    "        #     fit_sigmaX = True if simul_type == 3 else False  # If we fit variance given X_t\n",
    "        #     if simul_type == 1:\n",
    "        #         Data_dict = get_new_data_simple(\n",
    "        #             num_pts=2000, alpha=0.9, beta=0.9)\n",
    "        #         data_name = 'simulation_state_space'\n",
    "        #     if simul_type == 2:\n",
    "        #         data_container = utils_latest.data_loader()\n",
    "        #         _, Data_dict = data_container.get_non_stationary_simulate()\n",
    "        #         Data_dict['X'] = torch.from_numpy(Data_dict['X']).float()\n",
    "        #         Data_dict['Y'] = torch.from_numpy(Data_dict['Y']).float()\n",
    "        #         data_name = 'simulate_nonstationary'\n",
    "        #     if simul_type == 3:\n",
    "        #         # NOTE: somehow for this case, currently RF quantile regression does not yield shorter interval. We may tune past window to get different results (like decrease it to 250) if need\n",
    "        #         Data_dict = get_new_data()\n",
    "        #         data_name = 'simulate_heteroskedastic'\n",
    "        #     X_full, Y_full = Data_dict['X'].to(\n",
    "        #         device), Data_dict['Y'].to(device)\n",
    "        #     B = 20\n",
    "        #     past_window = 500\n",
    "        #     fit_func = None  # It is MLP above\n",
    "        # else:\n",
    "        #     data_name = 'solar'  # 'electric' or 'solar'\n",
    "        #     if data_name == 'solar':\n",
    "        #         # Get solar data WITH time t as covariate\n",
    "        #         dloader = utils_latest.data_loader()\n",
    "        #         Y_full, X_full_old, X_full_nonstat = dloader.get_non_stationary_real(\n",
    "        #             univariate=False)\n",
    "        #         if non_stat_solar:\n",
    "        #             X_full = X_full_nonstat\n",
    "        #         else:\n",
    "        #             X_full = X_full_old\n",
    "        #         fit_func = RandomForestRegressor(n_estimators=10, criterion='mse',\n",
    "        #                                           bootstrap=False, n_jobs=-1)\n",
    "        #         past_window = 500\n",
    "        #     if data_name == 'electric':\n",
    "        #         X_full, Y_full = electric_dataset()\n",
    "        #         fit_func = RandomForestRegressor(n_estimators=10, max_depth=1, criterion='mse',\n",
    "        #                                           bootstrap=False, n_jobs=-1)\n",
    "        #         past_window = 300\n",
    "        #     Y_full, X_full = torch.from_numpy(Y_full).float().to(\n",
    "        #         device), torch.from_numpy(X_full).float().to(device)\n",
    "        #     fit_sigmaX = False\n",
    "        #     B = 25\n",
    "\n",
    "        \n",
    "        \n",
    "        # data_name = 'solar'  # 'electric' or 'solar'\n",
    "        data_name = 'sepsis'\n",
    "        if data_name == 'sepsis':\n",
    "            sepsis = pd.read_csv('./Data/fully_imputed.csv')\n",
    "            Y_full = sepsis['hour2sepsis']\n",
    "            X_full = sepsis.drop(columns=['pat_id', 'hour2sepsis'])\n",
    "        if data_name == 'solar':\n",
    "            # Get solar data WITH time t as covariate\n",
    "            dloader = utils_latest.data_loader()\n",
    "            Y_full, X_full_old, X_full_nonstat = dloader.get_non_stationary_real(\n",
    "                univariate=False)\n",
    "            if non_stat_solar:\n",
    "                X_full = X_full_nonstat\n",
    "            else:\n",
    "                X_full = X_full_old\n",
    "            fit_func = RandomForestRegressor(n_estimators=10, criterion='mse',bootstrap=False, n_jobs=-1)\n",
    "            past_window = 500\n",
    "        if data_name == 'electric':\n",
    "            X_full, Y_full = electric_dataset()\n",
    "            fit_func = RandomForestRegressor(n_estimators=10, max_depth=1, criterion='mse',bootstrap=False, n_jobs=-1)\n",
    "            past_window = 300\n",
    "        Y_full, X_full = torch.from_numpy(Y_full).float().to(\n",
    "                device), torch.from_numpy(X_full).float().to(device)\n",
    "        fit_sigmaX = False\n",
    "        B = 25\n",
    "        \n",
    "        \n",
    "        alpha, itrial = 0.1, 0\n",
    "        N = int(X_full.shape[0]*train_frac)\n",
    "        X_train, X_predict, Y_train, Y_predict = X_full[:\n",
    "                                                        N], X_full[N:], Y_full[:N], Y_full[N:]\n",
    "\n",
    "        # Train\n",
    "        EnbPI = prediction_interval_with_SPCI(\n",
    "            X_train, X_predict, Y_train, Y_predict, fit_func=fit_func)\n",
    "        EnbPI.fit_bootstrap_models_online(B, fit_sigmaX=fit_sigmaX)\n",
    "        # Under cond quantile, we are ALREADY using the last window for prediction so smallT is True\n",
    "        smallT = not use_quantile_regr\n",
    "        EnbPI.compute_PIs_Ensemble_online(\n",
    "            alpha, smallT=smallT, past_window=past_window, use_quantile_regr=use_quantile_regr,\n",
    "            quantile_regr=quantile_regr)\n",
    "        results = EnbPI.get_results(alpha, data_name, itrial)\n",
    "        result_cov.append(results['coverage'].item())\n",
    "        result_width.append(results['width'].item())\n",
    "        # Lastly, compute rolling width to plot\n",
    "        PI = EnbPI.PIs_Ensemble\n",
    "        if use_quantile_regr:\n",
    "            PIs_SPCI = PI\n",
    "        else:\n",
    "            PIs_EnbPI = PI\n",
    "        Ytest = EnbPI.Y_predict.cpu().detach().numpy()\n",
    "        coverage = ((np.array(PI['lower']) <= Ytest)\n",
    "                    & (np.array(PI['upper']) >= Ytest))\n",
    "        width = ((np.array(PI['upper'])-np.array(PI['lower'])))\n",
    "        # cov_moving = rolling_avg(coverage)\n",
    "        # width_moving = rolling_avg(width)\n",
    "        dict_rolling[f'{name}{np.round(train_frac,2)}'] = np.vstack([\n",
    "            coverage, width])\n",
    "        if save_dict_rolling:\n",
    "            with open(f'EnbPI_rolling_{data_name}.p', 'wb') as fp:\n",
    "                pickle.dump(dict_rolling, fp,\n",
    "                            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # if simulation:\n",
    "        #     # # Examine recovery of F and Sigma\n",
    "        #     fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        #     ax[0, 0].plot(Data_dict['f(X)'])\n",
    "        #     ax[0, 1].plot(\n",
    "        #         EnbPI.Ensemble_pred_interval_centers.cpu().detach().numpy())\n",
    "        #     ax[1, 0].plot(Data_dict['Eps'])\n",
    "        #     ax[1, 1].plot(EnbPI.Ensemble_online_resid)\n",
    "        #     titles = [r'True $f(X)$', r'Est $f(X)$',\n",
    "        #               r'True $\\epsilon$', r'Est $\\epsilon$']\n",
    "        #     fig.tight_layout()\n",
    "        #     for i, ax_i in enumerate(ax.flatten()):\n",
    "        #         ax_i.set_title(titles[i])\n",
    "        #     fig.tight_layout()\n",
    "        #     plt.show()\n",
    "        #     plt.close()\n",
    "    dict_full[name] = np.vstack([result_cov, result_width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPWsYs6Dz8UT"
   },
   "source": [
    "Due to variability, what you see for EnbPI may be different. However, in general EnbPI intervals are wider\n",
    "\n",
    "0.8 indicates that training fraction as a percentage of total data is 80\\% or 1600 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6yJdKh8wWxY",
    "outputId": "163ad1de-9c1a-49a2-f965-0c910b84bfa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0.8\n",
      "EnbPI coverage   0.885000\n",
      "EnbPI width     48.889998\n",
      "SPCI coverage    0.912500\n",
      "SPCI width      43.294597\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "{} &    0.8 \\\\\n",
      "\\midrule\n",
      "EnbPI coverage &   0.88 \\\\\n",
      "EnbPI width    &  48.89 \\\\\n",
      "SPCI coverage  &   0.91 \\\\\n",
      "SPCI width     &  43.29 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_to_latex(dict_full)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
